{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "accd2e8f-2992-4e9e-89b0-cb10ce76cf69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.31.0\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2024.8.30)\n",
      "Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-no-record-file\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Cannot uninstall tokenizers 0.21.0\n",
      "\u001b[31m╰─>\u001b[0m The package's contents are unknown: no RECORD file was found for tokenizers.\n",
      "\n",
      "\u001b[1;36mhint\u001b[0m: You might be able to recover from this via: \u001b[32mpip install --force-reinstall --no-deps tokenizers==0.21.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.31.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a802e-4efe-4b30-969d-70b2325af7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a863c54-0777-4426-98e0-a4cba1611d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub==0.24.5 in ./namematchenv/lib/python3.11/site-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in ./namematchenv/lib/python3.11/site-packages (from huggingface-hub==0.24.5) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./namematchenv/lib/python3.11/site-packages (from huggingface-hub==0.24.5) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./namematchenv/lib/python3.11/site-packages (from huggingface-hub==0.24.5) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./namematchenv/lib/python3.11/site-packages (from huggingface-hub==0.24.5) (6.0.2)\n",
      "Requirement already satisfied: requests in ./namematchenv/lib/python3.11/site-packages (from huggingface-hub==0.24.5) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./namematchenv/lib/python3.11/site-packages (from huggingface-hub==0.24.5) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./namematchenv/lib/python3.11/site-packages (from huggingface-hub==0.24.5) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./namematchenv/lib/python3.11/site-packages (from requests->huggingface-hub==0.24.5) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./namematchenv/lib/python3.11/site-packages (from requests->huggingface-hub==0.24.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./namematchenv/lib/python3.11/site-packages (from requests->huggingface-hub==0.24.5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./namematchenv/lib/python3.11/site-packages (from requests->huggingface-hub==0.24.5) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface-hub==0.24.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4bccd275-e195-44aa-9b50-3ace4bb5256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef89c739-a5ac-4fd1-80ab-a339adc14476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.31.0\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "65b1a3cc-0fb6-4757-ac01-318d97d66e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertForSTS(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BertForSTS, self).__init__()\n",
    "        self.bert = models.Transformer('bert-base-uncased', max_seq_length=128)\n",
    "        self.pooling_layer = models.Pooling(self.bert.get_word_embedding_dimension())\n",
    "        self.sts_bert = SentenceTransformer(modules=[self.bert, self.pooling_layer])\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        output = self.sts_bert(input_data)['sentence_embedding']\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "58af3ca6-63f1-4cc5-b407-c1bce5ec7590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class STSBDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "\n",
    "        similarity_scores = [i['labels'] for i in dataset]\n",
    "        self.normalized_similarity_scores = [i/5.0 for i in similarity_scores]\n",
    "        self.first_sentences = [i['name1'] for i in dataset]\n",
    "        self.second_sentences = [i['name2'] for i in dataset]\n",
    "        self.concatenated_sentences = [[str(x), str(y)] for x,y in zip(self.first_sentences, self.second_sentences)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.concatenated_sentences)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return torch.tensor(self.normalized_similarity_scores[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        return tokenizer(self.concatenated_sentences[idx], padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "\n",
    "def collate_fn(texts):\n",
    "    input_ids = texts['input_ids']\n",
    "    attention_masks = texts['attention_mask']\n",
    "    features = [{'input_ids': input_id, 'attention_mask': attention_mask}\n",
    "                for input_id, attention_mask in zip(input_ids, attention_masks)]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "03554893-da32-46ff-98c5-2c6e0d62a3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8906f404-f492-4cad-86c8-50f0de08662c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_similarity_embedding_model(sentence_pair):\n",
    "    \"\"\"\n",
    "    Predict similarity between a pair of sentences\n",
    "    \"\"\"\n",
    "    test_input = tokenizer(sentence_pair, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    test_input['input_ids'] = test_input['input_ids']\n",
    "    test_input['attention_mask'] = test_input['attention_mask']\n",
    "    del test_input['token_type_ids']\n",
    "    output = model(test_input)\n",
    "    sim = torch.nn.functional.cosine_similarity(output[0], output[1], dim=0).item()\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d2ce02f-d34c-4dc5-ad40-de8a40d7c1c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "    \n",
    "with open('tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "93db2d3a-c5aa-433e-9fd6-8e81f594c2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim = predict_similarity_embedding_model([\"manmeet singh\", \"singh manmeet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d2bb5367-c39c-4902-96e1-c0246849524e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7494423389434814"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c6d9d359-f141-4bb3-a765-172243b9fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = [\n",
    "    \"traders\",\"trading\", \"enterprise\", \"garments\", \"collection\", \"food\", \"clothes\", \n",
    "    \"glass\", \"fittings\", \"digital\", \"kirana\", \"medical\", \"agency\",\"tex\",'logistics',\n",
    "    \"security\", \"systems\", \"badges\", \"hospitality\", \"jewellers\",'lic','agent',\n",
    "    \"ready-made\", \"store\", \"hospital\", \"restaurant\", \"auto\", \"center\", \n",
    "    \"dairy\", \"home\", \"products\", \"services\", \"furniture\", \"hardware\", \n",
    "    \"pharmacy\", \"stationery\", \"treatments\", \"nutrition\", \"wellness\", \n",
    "    \"sweets\", \"resort\", \"kitchen\", \"clothing\", \"market\",'workshop','agency','consumer','amale' \n",
    "    \"poultry\", \"seeds\", \"pesticides\", \"sales\", \"cafe\", \"clinic\", 'project'\n",
    "    \"supermart\", \"distributors\", \"automobiles\", \"electricity\", \n",
    "    \"electronics\", \"general\", \"provision\", \"fertilizers\", \"agriculture\", \n",
    "    \"beverages\", \"textiles\", \"plumbing\", \"supplies\", \"handicrafts\", \n",
    "    \"construction\", \"medical\", \"bakery\", \"tissue\", \"cleaning\", \n",
    "    \"appliances\", \"homecare\", \"kitchenware\", \"decor\", \"glass and fittings\",\n",
    "    \"interiors\", \"shopping\", \"crafts\", \"tools\", \"wholesale\", \n",
    "    \"retail\", \"outlet\", \"merchants\", \"trade\", \"distribution\", \n",
    "    \"solutions\", \"innovation\", \"consultancy\", \"services\", \"equipment\", \n",
    "    \"manufacturing\", \"exports\", \"imports\", \"packaging\", \"network\", \n",
    "    \"consultants\", \"transport\", \"moving\", \"storage\", \"logistics\", \n",
    "    \"construction\", \"real estate\",'distributor','wines','hardware',\n",
    "    'plywood','company','craft','soda','station','mobile','brothers','gas','trad','plywood','hp'\n",
    "    \"brokerage\", \"management\", 'handloom','co.','tvs','marketing',\n",
    "    \"finance\", \"investment\", \"funding\", \"support\", \"technology\", \n",
    "    \"software\", \"applications\", \"digital marketing\", \"advertising\", \n",
    "    \"communication\", \"entertainment\", \"events\", \"tourism\", \"travel\", \n",
    "    \"transportation\", \"automotive\", \"services\", \"supply chain\", \n",
    "    \"fashion\", \"cosmetics\", \"beauty\", \"spa\", \"wellness\", \"glass and fitting\",\n",
    "    \"personal care\", \"gifts\", \"custom\", \"specialty\", \"craftsmanship\", \n",
    "    \"fashions\", \"motors\", \"enterprises\", \"garment\", \"cloth centre\", \"mart\", \n",
    "    \"foods\", \"silk and readymade\", \"wool centre\", \"jewellery\", \"mill\", \n",
    "    \"farms\", \"farm\", \"electrical\", \"egg centre\", \"centre\", \n",
    "    \"vegetable and fruits\", \"vegetables\", \"fruits\", \"pvt\", \"pvt ltd\", \n",
    "    \"limited\", \"solutions\", \"energies\", \"photo\", \"studio\", \"works\", \n",
    "    \"associates\", \"medico\", \"agencies\", \"diagnosis\", \"cool drinks\", \n",
    "    \"drinks\", \"care\", \"liquor\", \"automobiles\", \"materials\", \"diagnostics\", \n",
    "    \"provision\", \"trader\", \"farms\", \"farm\", \"stations\", \"restaurant\", \n",
    "    \"creations\", \"travels\", \"hardware\", \"printers\", \"graphics\", \n",
    "    \"fertilisers\", \"house\", \"studio\", \"private\", \"appliances\", \"steels\", \n",
    "    \"shop\", \"metals\", \"international\", \"jwellers\", \"corporation\", \n",
    "    \"dresses\", \"industries\", \"electricals\", \"company\", \"lim\", \"colddrinks\", \n",
    "    \"electron\", \"medicines\", \"llc\", \"computers\", \"hotel\", \"spa\", \n",
    "    \"cosmetics\", \"telecom\", \"sarees\", \"petroleums\", \"bhandar\",'store','stores', \n",
    "    \"surgical\", \"wines\", \"constructions\", \"shoppy\", \"lab\", \"builders\", \n",
    "    \"footwear\", \"wear\", \"shoe\", \"repair\", \"ventures\", \"paint\", \"depot\",'cake','chinies',\n",
    "    \"tent\", \"decorators\", \"communications\", \"pharmacy\", \"products\",'textile','CERAMIC','Pharmaceuticals','stores','sons',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d3613c2c-dbbd-4598-9d50-e2bf88b06d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(KEYWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a6fa9ca6-4a5a-4271-97cc-798bc6a14c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SPECIAL_CHAR_DOT_REGEX = r\"[.]\"\n",
    "SPECIAL_CHARS_REGEX = r\"[-+.^:,_/\\s]+\" \n",
    "SALUTATION_REGEX = r\"^(shree|shri|miss|smt|mrs|mr|ms|dr|master|hon|sir|madam|prof|capt|major|rev|fr|br)\\s*\"\n",
    "PARENT_SPOUSE_NAME_REGEX = r\"(?:\\s*(?:s/o|d/o|w/o|so|do|wo|daughter of|son of|wife of|husband of)\\s*)\"\n",
    "COMMON_MUSLIM_SALUTATIONS_MOHAMMAD_REGEX = r\"\\b(mohammad|mohammed|muhamed|mohd|mohamed|mohamad|muhamad|muhammad|muhammed|muhammet|mohamud|mohummad|mohummed|mouhamed|muhamaad|mohammod|mouhamad|mo|md|mahmood|mahmud|ahmad|ahmed|hameed|hamid|hammed|mahd|mahmod|mohd|mouhammed|mohamad|muhmood|mohhammed|muhmamed|mohmed|mohmat|muhmat|mu|m|shaikh|mo)\\b\"\n",
    "LAST_NAMES_AGARWAL_VARIANTS_REGEX = r\"\\b(aggarwal|agrawal|agarwal|aggrawal|agarwalla|agarwal)\\b\"\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_lower(name):\n",
    "    if not isinstance(name, str):\n",
    "        name = str(name) if name is not None else \"\"\n",
    "    return name.lower()\n",
    "\n",
    "def replace_adjacent_duplicates(value):\n",
    "    if isinstance(value, str):\n",
    "        return re.sub(r'(.)\\1+', r'\\1', value)\n",
    "    return value\n",
    "\n",
    "def replace_characters(name):\n",
    "    replacements = {'e': 'i', 'j': 'z', 'v': 'w', 'q': 'k'}\n",
    "    for old, new in replacements.items():\n",
    "        name = name.replace(old, new)\n",
    "    return name\n",
    "\n",
    "def replace_bigrams(name):\n",
    "    replacements = {'ph': 'f', 'gh': 'g', 'th': 't', 'kh': 'k', 'dh': 'd', 'ch': 'c', 'sh': 's', 'au': 'o',\n",
    "                    'bh': 'b', 'ks': 'x', 'ck': 'k', 'ah': 'h', 'wh': 'w', 'wr': 'r'}\n",
    "    for old, new in replacements.items():\n",
    "        name = name.replace(old, new)\n",
    "    return name\n",
    "\n",
    "def remove_extra_spaces(name):\n",
    "    return re.sub(r'\\s+', ' ', name).strip()\n",
    "\n",
    "def remove_consonant_a(name):\n",
    "    consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "    new_name = ''.join([name[i] for i in range(len(name)) if not (i > 0 and name[i] == 'a' and name[i - 1].lower() in consonants)])\n",
    "    return new_name\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(SPECIAL_CHAR_DOT_REGEX, '', text)\n",
    "    text = re.sub(SPECIAL_CHARS_REGEX, '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_salutations(text):\n",
    "    return re.sub(SALUTATION_REGEX, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "def remove_parent_spouse_name(text):\n",
    "    return re.sub(r'\\s*(?:s[\\s./]*o|d[\\s./]*o|w[\\s./]*o|son[\\s]*of|daughter[\\s]*of|wife[\\s]*of|husband[\\s]*of|child[\\s]*of)\\s+[\\w\\s,.]*$', '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "def remove_common_muslim_variations(text):\n",
    "    return re.sub(COMMON_MUSLIM_SALUTATIONS_MOHAMMAD_REGEX, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "def remove_agarwal_variants(text):\n",
    "    return re.sub(LAST_NAMES_AGARWAL_VARIANTS_REGEX, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = ['devi', 'dei', 'debi', 'kmr', 'kumr','bhai', 'bhau', 'bai', 'ben', 'kaur', 'Md', 'Mohd', 'Mohammad', 'Mohamad','alam','shekh','sek']\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5359f1a-6f7a-4d0f-8eb7-64c003fdfada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bca66280-e8e0-446a-8a86-cc2dbcc1fffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_layer1(name):\n",
    "    name = str(name) if name is not None else \"\"\n",
    "    name = convert_to_lower(name)\n",
    "    name = remove_salutations(name)\n",
    "    name = remove_parent_spouse_name(name)\n",
    "    name = remove_stop_words(name)\n",
    "    name = remove_extra_spaces(name)\n",
    "    return name\n",
    "\n",
    "def check_keywords_layer1(name1, name2):\n",
    "    name1 = str(name1) if name1 is not None else \"\"\n",
    "    name2 = str(name2) if name2 is not None else \"\"\n",
    "\n",
    "    found_in_name1 = any(keyword in name1.lower() for keyword in KEYWORDS)\n",
    "    found_in_name2 = any(keyword in name2.lower() for keyword in KEYWORDS)\n",
    "\n",
    "    if found_in_name1 and found_in_name2:\n",
    "        return 1  \n",
    "    elif found_in_name1 or found_in_name2:\n",
    "        return 0\n",
    "    return 1\n",
    "    \n",
    "def calculate_fuzzy_similarity_layer1(name1, name2):\n",
    "    name1 = preprocess_layer1(name1)\n",
    "    name2 = preprocess_layer1(name2)\n",
    "    \n",
    "    if not isinstance(name1, str):\n",
    "        name1 = str(name1) if name1 is not None else \"\"\n",
    "    if not isinstance(name2, str):\n",
    "        name2 = str(name2) if name2 is not None else \"\"\n",
    "\n",
    "\n",
    "    fuzzy_ratio = fuzz.ratio(name1, name2) / 100.0\n",
    "    fuzzy_partial_ratio = fuzz.partial_ratio(name1, name2) / 100.0\n",
    "    fuzzy_token_sort_ratio = fuzz.token_sort_ratio(name1, name2) / 100.0\n",
    "\n",
    "    fuzzy_similarity = (fuzzy_ratio + fuzzy_partial_ratio + fuzzy_token_sort_ratio ) / 3.0\n",
    "    return fuzzy_similarity\n",
    "\n",
    "def fuzzy_layer1(name1, name2):\n",
    "    fuzzy_SS = calculate_fuzzy_similarity_layer1(name1, name2)\n",
    "    Prediction = 0  \n",
    "    if fuzzy_SS >= 0.80:  \n",
    "        keyword_flag = check_keywords_layer1(name1, name2)\n",
    "        Prediction = 1 if keyword_flag == 1 else 0  \n",
    "    return fuzzy_SS, Prediction\n",
    "#-----------------------------------------Data Preprocessing anf Framework--------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_layer2(name):\n",
    "    name = remove_salutations(name)\n",
    "    name = remove_parent_spouse_name(name)\n",
    "    name = remove_common_muslim_variations(name)\n",
    "    name = remove_agarwal_variants(name)\n",
    "    name = convert_to_lower(name)\n",
    "    name = replace_adjacent_duplicates(name)\n",
    "    name = replace_characters(name)\n",
    "    name = replace_bigrams(name)\n",
    "    name = remove_consonant_a(name)\n",
    "    name = remove_special_characters(name)\n",
    "    name = remove_extra_spaces(name)\n",
    "    name = remove_stop_words(name)\n",
    "    return name\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "    \n",
    "with open('tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "    \n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "def calculate_levenshtein_similarity(name1, name2):\n",
    "    lev_distance = levenshtein_distance(name1, name2)\n",
    "    max_len = max(len(name1), len(name2))\n",
    "    return (max_len - lev_distance) / max_len if max_len > 0 else 1.0\n",
    "\n",
    "def calculate_phonetic_similarity(name1, name2):\n",
    "    soundex1 = jellyfish.soundex(name1)\n",
    "    soundex2 = jellyfish.soundex(name2)\n",
    "    return jellyfish.jaro_winkler_similarity(soundex1, soundex2)\n",
    "\n",
    "def calculate_jaccard_similarity(name1, name2):\n",
    "    set1, set2 = set(name1), set(name2)\n",
    "    intersection, union = set1.intersection(set2), set1.union(set2)\n",
    "    return len(intersection) / len(union) if union else 1.0\n",
    "\n",
    "##------------------------------------Calling Name_Match------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "# def name_match(name1, name2):\n",
    "#     name1_processed = preprocess_layer2(name1)\n",
    "#     name2_processed = preprocess_layer2(name2)\n",
    "\n",
    "#     embedding_similarity = predict_similarity_embedding_model([name1_processed, name2_processed])\n",
    "#     levenshtein_similarity = calculate_levenshtein_similarity(name1_processed, name2_processed)\n",
    "#     phonetic_similarity = calculate_phonetic_similarity(name1_processed, name2_processed)\n",
    "#     jaccard_similarity = calculate_jaccard_similarity(name1_processed, name2_processed)\n",
    "\n",
    "#     fuzzy_ratio = fuzz.ratio(name1_processed, name2_processed) / 100.0\n",
    "#     fuzzy_partial_ratio = fuzz.partial_ratio(name1_processed, name2_processed) / 100.0\n",
    "#     fuzzy_token_sort_ratio = fuzz.token_sort_ratio(name1_processed, name2_processed) / 100.0\n",
    "\n",
    "#     fuzzy_similarity = (fuzzy_ratio + fuzzy_partial_ratio + fuzzy_token_sort_ratio ) / 3.0\n",
    "\n",
    "#     final_score = (\n",
    "#         embedding_similarity * 0.40 +\n",
    "#         phonetic_similarity * 0.30 +\n",
    "#         jaccard_similarity * 0.30 \n",
    "#     )\n",
    "  \n",
    "\n",
    "#     return {\n",
    "#         \"name1\": name1,\n",
    "#         \"name2\": name2,\n",
    "#         \"embedding_similarity\": embedding_similarity,\n",
    "#         \"phonetic_similarity\": phonetic_similarity,\n",
    "#         \"jaccard_similarity\": jaccard_similarity,\n",
    "#         \"final_score\": final_score\n",
    "#     }\n",
    "# Define the list of keywords\n",
    "\n",
    "def check_keywords_and_set_prediction(name1, name2, final_score, threshold=0.65):\n",
    "    # Check for keywords in each name\n",
    "    found_in_name1 = any(keyword in name1.lower() for keyword in KEYWORDS)\n",
    "    found_in_name2 = any(keyword in name2.lower() for keyword in KEYWORDS)\n",
    "\n",
    "    if found_in_name1 and found_in_name2:\n",
    "        return 1\n",
    "    elif found_in_name1 or found_in_name2:\n",
    "        return 0\n",
    "    return 1 if final_score > threshold else 0\n",
    "\n",
    "def name_match(name1, name2):\n",
    "    # Preprocess the names\n",
    "    name1_processed = preprocess_layer2(name1)\n",
    "    name2_processed = preprocess_layer2(name2)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    embedding_similarity = predict_similarity_embedding_model([name1_processed, name2_processed])\n",
    "    levenshtein_similarity = calculate_levenshtein_similarity(name1_processed, name2_processed)\n",
    "    phonetic_similarity = calculate_phonetic_similarity(name1_processed, name2_processed)\n",
    "    jaccard_similarity = calculate_jaccard_similarity(name1_processed, name2_processed)\n",
    "\n",
    "    # Calculate fuzzy similarity metrics\n",
    "    fuzzy_ratio = fuzz.ratio(name1_processed, name2_processed) / 100.0\n",
    "    fuzzy_partial_ratio = fuzz.partial_ratio(name1_processed, name2_processed) / 100.0\n",
    "    fuzzy_token_sort_ratio = fuzz.token_sort_ratio(name1_processed, name2_processed) / 100.0\n",
    "    fuzzy_similarity = (fuzzy_ratio + fuzzy_partial_ratio + fuzzy_token_sort_ratio) / 3.0\n",
    "\n",
    "    # Compute the final score with weights\n",
    "    final_score = (\n",
    "        embedding_similarity * 0.286461 +\n",
    "        phonetic_similarity * 0.123011 +\n",
    "        jaccard_similarity * 0.590528\n",
    "    )\n",
    "    # Determine prediction based on keywords and final score\n",
    "    prediction = check_keywords_and_set_prediction(name1, name2, final_score)\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        \"name1\": name1,\n",
    "        \"name2\": name2,\n",
    "        \"embedding_similarity\": embedding_similarity,\n",
    "        \"phonetic_similarity\": phonetic_similarity,\n",
    "        \"jaccard_similarity\": jaccard_similarity,\n",
    "        \"final_score\": final_score,\n",
    "        \"Prediction\": prediction\n",
    "    }\n",
    "\n",
    "\n",
    "# ##------------------------------------Fuzzy With Data Preprocessing and keyword matching-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def preprocess_FuzzyWuzzy(name):\n",
    "    name = convert_to_lower(name)\n",
    "    name = remove_salutations(name)\n",
    "    name = remove_parent_spouse_name(name)\n",
    "    name = remove_stop_words(name)\n",
    "    name = remove_extra_spaces(name)\n",
    "    return name\n",
    "\n",
    "def calculate_fuzzy_similarity_processed(name1, name2):\n",
    "    name1 = preprocess_FuzzyWuzzy(name1)\n",
    "    name2 = preprocess_FuzzyWuzzy(name2)\n",
    "    \n",
    "    fuzzy_ratio = fuzz.ratio(name1, name2) / 100.0\n",
    "    fuzzy_partial_ratio = fuzz.partial_ratio(name1, name2) / 100.0\n",
    "    fuzzy_token_sort_ratio = fuzz.token_sort_ratio(name1, name2) / 100.0\n",
    "\n",
    "    fuzzy_similarity = (fuzzy_ratio + fuzzy_partial_ratio + fuzzy_token_sort_ratio) / 3.0\n",
    "    return fuzzy_similarity\n",
    " \n",
    "\n",
    "# def check_keywords(name1, name2, KEYWORDS):\n",
    "#     name1 = name1.lower()\n",
    "#     name2 = name2.lower()\n",
    "#     for keyword in KEYWORDS:\n",
    "#         if keyword in name1 or keyword in name2:\n",
    "#             return 0  \n",
    "#     return 1\n",
    "\n",
    "def check_keywords(name1, name2, keywords):\n",
    "    name1 = str(name1) if name1 is not None else \"\"\n",
    "    name2 = str(name2) if name2 is not None else \"\"\n",
    "\n",
    "    found_in_name1 = any(keyword in name1.lower() for keyword in KEYWORDS)\n",
    "    found_in_name2 = any(keyword in name2.lower() for keyword in KEYWORDS)\n",
    "\n",
    "    if found_in_name1 and found_in_name2:\n",
    "        return 1  \n",
    "    elif found_in_name1 or found_in_name2:\n",
    "        return 0\n",
    "    return 1\n",
    "    \n",
    "def process_false_cases(row):\n",
    "    name1 = row['name1']\n",
    "    name2 = row['name2']\n",
    "\n",
    "    fuzzy_SS = calculate_fuzzy_similarity_processed(name1, name2)\n",
    "\n",
    "    fuzzy_flag = 1 if fuzzy_SS >= 0.70 else 0\n",
    "\n",
    "    if fuzzy_flag:\n",
    "        keyword_adjustment = check_keywords(name1, name2, KEYWORDS)\n",
    "        fuzzy_flag = 1 if keyword_adjustment else 0\n",
    "\n",
    "    return {\n",
    "        \"Third_Layer_Score\": fuzzy_SS,\n",
    "        \"Prediction\": int(fuzzy_flag),\n",
    "        \"Third_Layer_Flag\": fuzzy_flag  \n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "129f023e-9ecc-4310-b554-877c44baf2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSTS(\n",
       "  (bert): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (pooling_layer): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (sts_bert): SentenceTransformer(\n",
       "    (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a84d695c-853e-4cd8-8691-096acc96a8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttttttttttttt\n",
      "                               name1                            name2\n",
      "13                 N NIYAMATH NISHA        NIYAMATH NISHA NAVAS BASHA\n",
      "14        Niyamath Nisha Navas Basha                N NIYAMATH NISHA \n",
      "16                            Adarsh               Aadarsh Dhanotiya \n",
      "28      DESHMUKH SANTOSH NARAYANRAO   Mr. SANTOSH NARAYANRAO DESHMUKH\n",
      "33                          ROHITASH              Mr. ROHITASH  MEENA\n",
      "...                              ...                              ...\n",
      "113142               MAYASINGH DAWAR                     AAKASH DAWAR\n",
      "113143               MAYASINGH DAWAR                     AAKASH DAWAR\n",
      "113145                  Kavita meena              KAVITA SAREE CENTRE\n",
      "113146                  Kavita meena              KAVITA SAREE CENTRE\n",
      "113147                       SHABINA               COLORS COLLECTIONS\n",
      "\n",
      "[29775 rows x 2 columns]\n",
      "\n",
      "Confusion Matrix for Each Layer:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "    df_layer1 = pd.read_csv(file_path)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    df_layer1[\"Prediction\"] = 0  \n",
    "\n",
    "    df_layer1[[\"First_Layer_Score\", \"first_layer_pass\"]] = df_layer1.apply(\n",
    "        lambda row: pd.Series([\n",
    "            fuzzy_layer1(row['name1'], row['name2'])[0],  \n",
    "            fuzzy_layer1(row['name1'], row['name2'])[0] >= threshold  \n",
    "        ]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df_layer1[\"Prediction\"] = df_layer1[\"first_layer_pass\"].astype(int)  \n",
    "\n",
    "    df_layer1.to_csv(\"first_audit.csv\", index=False)\n",
    "\n",
    "    df_layer2 = df_layer1[df_layer1[\"Prediction\"] == 0].copy()\n",
    "    \n",
    "    print(\"tttttttttttttt\")\n",
    "    print(df_layer2[['name1','name2']])\n",
    "\n",
    "    df_layer2 = df_layer1[df_layer1[\"Prediction\"] == 0].copy()\n",
    "\n",
    "    # Ensure names are strings\n",
    "    df_layer2['name2'] = df_layer2['name2'].astype(str)\n",
    "    df_layer2['name1'] = df_layer2['name1'].astype(str)\n",
    "\n",
    "    # Apply name matching logic directly\n",
    "    name_match_results = df_layer2.apply(lambda row: name_match(row['name1'], row['name2']), axis=1)\n",
    "\n",
    "    # Extract similarity scores and assign directly\n",
    "    df_layer2 = df_layer2.assign(\n",
    "        embedding_similarity=[result[\"embedding_similarity\"] for result in name_match_results],\n",
    "        phonetic_similarity=[result[\"phonetic_similarity\"] for result in name_match_results],\n",
    "        jaccard_similarity=[result[\"jaccard_similarity\"] for result in name_match_results],\n",
    "        final_score=[result[\"final_score\"] for result in name_match_results],\n",
    "        Prediction=[result[\"Prediction\"] for result in name_match_results]  # Prediction is calculated inside name_match\n",
    "    )\n",
    "\n",
    "    # Save Layer 2 audit results\n",
    "    df_layer2.to_csv(\"second_audit.csv\", index=False)\n",
    "\n",
    "\n",
    "    df_layer3 = df_layer2[df_layer2[\"Prediction\"] == 0].copy()\n",
    "    results = df_layer3.apply(process_false_cases, axis=1)\n",
    "    \n",
    "    df_layer3[\"Third_Layer_Score\"] = results.apply(lambda x: x[\"Third_Layer_Score\"])\n",
    "    df_layer3[\"Prediction\"] = results.apply(lambda x: x[\"Prediction\"])\n",
    "    df_layer3[\"Third_Layer_Flag\"] = results.apply(lambda x: x[\"Third_Layer_Flag\"])\n",
    "    \n",
    "    df_layer3.to_csv(\"third_audit.csv\", index=False)\n",
    "\n",
    "    df_combined = pd.concat([\n",
    "        df_layer1[df_layer1[\"Prediction\"] == 1],\n",
    "        df_layer2[df_layer2[\"Prediction\"] == 1],\n",
    "        df_layer3[df_layer3[\"Prediction\"] == 1],\n",
    "        df_layer3[df_layer3[\"Prediction\"] == 0]  \n",
    "    ], ignore_index=True)\n",
    "\n",
    "    df_combined.to_csv(\"RF_1WEEK(80-65-70).csv\", index=False)\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    combined_data = process_name_matching(\"one_week_m.csv\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for Each Layer:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8776c68d-8620-4abd-afda-f78897d94239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_first=pd.read_csv('first_audit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0826c407-b99f-458c-8d87-8d24933253b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499519, 16)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_first.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7962a844-2a8a-41ca-9a10-e0ead4088e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction\n",
       "1    416022\n",
       "0     83497\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_first['Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7ab15283-45fd-4836-b1b5-9ae32408783d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_second_audit=pd.read_csv('second_audit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "db30ccda-ebe3-4b9b-87fb-bc11b7ee6948",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29775, 20)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_second_audit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6bd14c54-c5dd-4e9e-8002-b8ada8174d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction\n",
       "0    19148\n",
       "1    10627\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_second_audit['Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e59b46f8-98d9-47e8-80e4-d2720d1a6e91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_third_audit=pd.read_csv(\"third_audit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a8203266-f8db-46dc-960e-5dfaa554358b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19148, 22)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_third_audit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "07c4fe92-00e8-40f5-8f29-fb705db720f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction\n",
       "0    15750\n",
       "1     3398\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_third_audit['Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f30638-cc6b-4865-ae12-305ca6112abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91264acb-5639-4d8c-99d4-e268c5cab805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "836c3bea-a1ec-4601-8988-47e67b41365d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_384635/380943336.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_2=pd.read_csv(\"one_month_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_2=pd.read_csv(\"one_month_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "06da28c3-70e0-42a1-8197-aa4580a6856e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499519, 13)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6098e49-a8af-4404-bbb8-68be2c4c1ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_1975632/794012338.py:1: DtypeWarning: Columns (10,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_21=pd.read_csv(\"check_experiment6.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_21=pd.read_csv(\"check_experiment6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da8056c-299e-49bc-960d-82f76471793e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499519, 23)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_21.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bf02d9d-6796-44e5-8ad6-8251ec05535c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction\n",
       "1    436169\n",
       "0     63350\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_21['Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "68ccbe27-4830-4743-ba5c-70c9fe8f847f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_384635/2125093464.py:1: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_211=pd.read_csv(\"check_experiment4.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_211=pd.read_csv(\"check_experiment4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "baa3482c-12b5-4c88-8440-fb3a907ccfd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'merchantId', 'consumerId', 'szScore', 'szFlag', 'hvScore',\n",
       "       'hvFlag', 'name2', 'name1', 'dsScore', 'dsFlag', 'flScore', 'flFlag',\n",
       "       'Prediction', 'First_Layer_Score', 'first_layer_pass',\n",
       "       'embedding_similarity', 'phonetic_similarity', 'jaccard_similarity',\n",
       "       'final_score', 'Second_Layer_Pass', 'Third_Layer_Score',\n",
       "       'Third_Layer_Flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_211.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae518962-1b92-43c8-8e6e-b28d2d06fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flFlag,dsFlag,hvFlag,Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a9ec814a-e2f0-474a-8c02-fe4c643618ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction\n",
       "1    98528\n",
       "0    14620\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_211['Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e8c6bd-0079-4ad2-9e66-117d71806004",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3075690/1458054.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(\n",
      "/var/tmp/ipykernel_3075690/1458054.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Score\"] = pd.to_numeric(df_layer2[\"Second_Layer_Score\"], errors='coerce')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0,\\n       1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\\n       1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\\n       0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\\n      dtype='float64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 137\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_combined\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Process the data and get combined results\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     combined_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_name_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConfusion Matrix for Each Layer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 125\u001b[0m, in \u001b[0;36mprocess_name_matching\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    121\u001b[0m df_layer3\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthird_audit.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Combine results\u001b[39;00m\n\u001b[1;32m    124\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mdf_layer1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_layer1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    126\u001b[0m     df_layer2[df_layer2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m    127\u001b[0m     df_layer3[df_layer3[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    128\u001b[0m ], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m df_combined\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone_week_result.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_combined\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5937\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0,\\n       1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\\n       1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\\n       0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\\n      dtype='float64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# # from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "# def process_name_matching(file_path):\n",
    "#     df_layer1 = pd.read_csv(file_path)\n",
    "#     threshold = 0.75\n",
    "#     threshold1 = 0.65\n",
    "    \n",
    "    \n",
    "#     df_layer1[\"Prediction\"] = 0\n",
    "\n",
    "# #     df_layer1[\"First_Layer_Pass\"] = df_layer1.apply(fuzzy_layer1, axis=1)\n",
    "# #     df_layer1[\"Prediction\"] = df_layer1.apply(lambda x: 1 if x[\"First_Layer_Pass\"] else x[\"Prediction\"], axis=1)\n",
    "#     df_layer1[[\"First_Layer_Score\", \"Prediction\"]] = df_layer1.apply(\n",
    "#     lambda row: pd.Series(fuzzy_layer1(row['name1'], row['name2'])),\n",
    "#     axis=1 )\n",
    "    \n",
    "#     df_layer1.to_csv(\"first_audit.csv\", index=False)\n",
    "    \n",
    "#     df_layer2 = df_layer1[df_layer1[\"Prediction\"] == 0]  # Filter rows where Prediction == False\n",
    "#     df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "#     df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "#     df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n",
    "    \n",
    "#     df_layer2.to_csv(\"second_audit.csv\", index=False)\n",
    "\n",
    "#     # Third Layer\n",
    "#     df_layer3 = df_layer2[df_layer2[\"Prediction\"] == 0]  # Filter rows where Prediction == False\n",
    "#     df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "#     df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n",
    "    \n",
    "#     df_layer3.to_csv(\"third_audit.csv\", index=False)\n",
    "   \n",
    "#     df_combined = pd.concat([\n",
    "#         df_layer1[df_layer1[\"Prediction\"]],\n",
    "#         df_layer2[df_layer2[\"Prediction\"]],\n",
    "#         df_layer3[df_layer3[\"Prediction\"]]\n",
    "#     ], ignore_index=True)\n",
    "\n",
    "#     df_combined.to_csv(\"one_week_result.csv\", index=False)\n",
    "\n",
    "#     return df_combined\n",
    "\n",
    "#     # Final output\n",
    "#     df_layer1[\"Prediction\"] = 0\n",
    "#     df_layer1.loc[df_layer1[df_layer1[\"Prediction\"]].index, \"Prediction\"] = 1\n",
    "#     df_layer1.loc[df_layer2[df_layer2[\"Prediction\"]].index, \"Prediction\"] = 1\n",
    "#     df_layer1.loc[df_layer3[df_layer3[\"Prediction\"]].index, \"Prediction\"] = 1\n",
    "\n",
    "#     return df_combined\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     combined_data = process_name_matching(\"test.csv\")\n",
    "\n",
    "#     print(\"\\nConfusion Matrix for Each Layer:\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cae9ae-9308-4e01-affd-e5029a42d433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441539d-903c-4852-b416-741d3f8a619b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903f8fd-9fe0-4eab-bce5-415675d1627c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dae566-b291-4bf2-a011-d315c9718a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52fdd2-50d7-4eb7-9674-8e68474f0fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcf9a1-ba13-47ac-988e-fa82859b6237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09663483-4d3a-4fdc-941a-d3eb107e60f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad309b1-e1e1-476a-bae1-842499d00cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "# from fuzzywuzzy import fuzz\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# # from BB import BertForSTS\n",
    "\n",
    "# # threshold = 0.80\n",
    "# # threshold1 = 0.65\n",
    "\n",
    "\n",
    "\n",
    "# ##------------------------------------Fuzzy Wuzzy Layer-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def convert_to_lower(name):\n",
    "#     return name.lower()\n",
    "\n",
    "# def remove_extra_spaces(name):\n",
    "#     return re.sub(r'\\s+', ' ', name).strip()\n",
    "\n",
    "# def remove_stop_words(text):\n",
    "#     # stop_words = ['devi', 'dei', 'debi', 'kumar', 'kumaar', 'kumari', 'kumaari', 'kmr', 'kumr', 'bhai', 'bhau', 'bai', 'ben', 'singh', 'kaur', 'Md', 'Mohd', 'Mohammad', 'Mohamad','alam','shekh','sek']\n",
    "#     stop_words = ['devi', 'dei', 'debi', 'kmr', 'kumr', 'bhai', 'bhau', 'bai', 'ben', 'kaur', 'Md', 'Mohd', 'Mohammad', 'Mohamad','alam','shekh','sek']\n",
    "#     words = text.split()\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "# def preprocess_layer1(name):\n",
    "#     name = convert_to_lower(name)\n",
    "#     name = remove_extra_spaces(name)\n",
    "#     name = remove_stop_words(name)\n",
    "#     return name\n",
    "\n",
    "# def check_keywords_layer1(name1, name2):\n",
    "#     found_in_name1 = any(keyword in name1.lower() for keyword in KEYWORDS)\n",
    "#     found_in_name2 = any(keyword in name2.lower() for keyword in KEYWORDS)\n",
    "    \n",
    "#     if found_in_name1 and found_in_name2:\n",
    "#         return 1  \n",
    "#     elif found_in_name1 or found_in_name2:\n",
    "#         return 0 \n",
    "#     return 1  \n",
    "# def check_permutation_match(name1, name2):\n",
    "#     name1 = preprocess_layer1(name1).replace(\" \", \"\")\n",
    "#     name2 = preprocess_layer1(name2).replace(\" \", \"\")\n",
    "    \n",
    "#     return sorted(name1) == sorted(name2)\n",
    "\n",
    "# def calculate_fuzzy_similarity_layer1(name1, name2):\n",
    "#     name1 = preprocess_layer1(name1)\n",
    "#     name2 = preprocess_layer1(name2)\n",
    "    \n",
    "#     fuzzy_ratio = fuzz.ratio(name1, name2) / 100.0\n",
    "#     fuzzy_partial_ratio = fuzz.partial_ratio(name1, name2) / 100.0\n",
    "#     fuzzy_token_sort_ratio = fuzz.token_sort_ratio(name1, name2) / 100.0\n",
    "#     fuzzy_token_set_ratio = fuzz.token_set_ratio(name1, name2) / 100.0\n",
    "\n",
    "#     fuzzy_similarity = (fuzzy_ratio + fuzzy_partial_ratio + fuzzy_token_sort_ratio + fuzzy_token_set_ratio) / 4.0\n",
    "#     return fuzzy_similarity\n",
    "\n",
    "# def check_substring_match(name1, name2):\n",
    "#     name1 = preprocess_layer1(name1)\n",
    "#     name2 = preprocess_layer1(name2)\n",
    "#     words1 = set(name1.lower().split())\n",
    "#     words2 = set(name2.lower().split())\n",
    "    \n",
    "#     if words1.issubset(words2) or words2.issubset(words1):\n",
    "#         return 1\n",
    "#     return 0\n",
    "\n",
    "# def fuzzy_layer1(row):\n",
    "#     name1 = row['name1']\n",
    "#     name2 = row['name2']\n",
    "    \n",
    "#     if check_substring_match(name1, name2):\n",
    "#         keyword_flag = check_keywords_layer1(name1, name2)\n",
    "#         return 1 if keyword_flag == 1 else 0  # If keywords conflict, mark as not matching\n",
    "    \n",
    "#     if check_permutation_match(name1, name2):\n",
    "#         keyword_flag = check_keywords_layer1(name1, name2)\n",
    "#         return 1 if keyword_flag == 1 else 0  # If keywords conflict, mark as not matching\n",
    "    \n",
    "#     fuzzy_SS = calculate_fuzzy_similarity_layer1(name1, name2)\n",
    "#     fuzzy_flag = fuzzy_SS >= 0.80  # Consider fuzzy similarity >= 80% as a match\n",
    "    \n",
    "#     if fuzzy_flag:\n",
    "#         keyword_flag = check_keywords_layer1(name1, name2)\n",
    "#         return 1 if keyword_flag == 1 else 0  # If keywords conflict, mark as not matching\n",
    "\n",
    "#     return 0\n",
    "\n",
    "\n",
    "\n",
    "# #-----------------------------------------Data Preprocessing anf Framework--------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# import torch\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from Levenshtein import distance as levenshtein_distance\n",
    "# import jellyfish\n",
    "\n",
    "# SPECIAL_CHAR_DOT_REGEX = r\"[.]\"\n",
    "# SPECIAL_CHARS_REGEX = r\"[-+.^:,_/\\s]+\" \n",
    "# SALUTATION_REGEX = r\"^(shree|shri|miss|smt|mrs|mr|ms|dr|master|hon|sir|madam|prof|capt|major|rev|fr|br)\\s*\"\n",
    "# PARENT_SPOUSE_NAME_REGEX = r\"(?:\\s*(?:s/o|d/o|w/o|so|do|wo|daughter of|son of|wife of|husband of)\\s*)\"\n",
    "# COMMON_MUSLIM_SALUTATIONS_MOHAMMAD_REGEX = r\"\\b(mohammad|mohammed|muhamed|mohd|mohamed|mohamad|muhamad|muhammad|muhammed|muhammet|mohamud|mohummad|mohummed|mouhamed|muhamaad|mohammod|mouhamad|mo|md|mahmood|mahmud|ahmad|ahmed|hameed|hamid|hammed|mahd|mahmod|mohd|mouhammed|mohamad|muhmood|mohhammed|muhmamed|mohmed|mohmat|muhmat|mu|m|shaikh|mo)\\b\"\n",
    "# LAST_NAMES_AGARWAL_VARIANTS_REGEX = r\"\\b(aggarwal|agrawal|agarwal|aggrawal|agarwalla|agarwal)\\b\"\n",
    "\n",
    "\n",
    "# def convert_to_lower(name):\n",
    "#     return name.lower()\n",
    "\n",
    "# def replace_adjacent_duplicates(value):\n",
    "#     if isinstance(value, str):\n",
    "#         return re.sub(r'(.)\\1+', r'\\1', value)\n",
    "#     return value\n",
    "\n",
    "# def replace_characters(name):\n",
    "#     replacements = {'e': 'i', 'j': 'z', 'v': 'w', 'q': 'k'}\n",
    "#     for old, new in replacements.items():\n",
    "#         name = name.replace(old, new)\n",
    "#     return name\n",
    "\n",
    "# def replace_bigrams(name):\n",
    "#     replacements = {'ph': 'f', 'gh': 'g', 'th': 't', 'kh': 'k', 'dh': 'd', 'ch': 'c', 'sh': 's', 'au': 'o',\n",
    "#                     'bh': 'b', 'ks': 'x', 'ck': 'k', 'ah': 'h', 'wh': 'w', 'wr': 'r'}\n",
    "#     for old, new in replacements.items():\n",
    "#         name = name.replace(old, new)\n",
    "#     return name\n",
    "\n",
    "# def remove_extra_spaces(name):\n",
    "#     return re.sub(r'\\s+', ' ', name).strip()\n",
    "\n",
    "# def remove_consonant_a(name):\n",
    "#     consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "#     new_name = ''.join([name[i] for i in range(len(name)) if not (i > 0 and name[i] == 'a' and name[i - 1].lower() in consonants)])\n",
    "#     return new_name\n",
    "\n",
    "# def remove_special_characters(text):\n",
    "#     text = re.sub(SPECIAL_CHAR_DOT_REGEX, '', text)\n",
    "#     text = re.sub(SPECIAL_CHARS_REGEX, '', text)\n",
    "#     return text.strip()\n",
    "\n",
    "# def remove_salutations(text):\n",
    "#     return re.sub(SALUTATION_REGEX, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# def remove_parent_spouse_name(text):\n",
    "#     return re.sub(r'\\s*(?:s/o|d/o|w/o|so|do|wo|daughter of|son of|wife of|husband of|daughter|son|child of)\\s*[\\w\\s,.]*$', '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# def remove_common_muslim_variations(text):\n",
    "#     return re.sub(COMMON_MUSLIM_SALUTATIONS_MOHAMMAD_REGEX, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# def remove_agarwal_variants(text):\n",
    "#     return re.sub(LAST_NAMES_AGARWAL_VARIANTS_REGEX, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# def remove_stop_words(text):\n",
    "#     stop_words = ['devi', 'dei', 'debi', 'kmr', 'kumr', 'bhai', 'bhau', 'bai', 'ben', 'kaur', 'Md', 'Mohd', 'Mohammad', 'Mohamad','alam','shekh','sek']\n",
    "#     # stop_words = ['devi', 'dei', 'debi', 'kumar', 'kumaar', 'kumari', 'kumaari', 'kmr', 'kumr', 'bhai', 'bhau', 'bai', 'ben', 'singh', 'kaur', 'Md', 'Mohd', 'Mohammad', 'Mohamad']\n",
    "#     words = text.split()\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return ' '.join(filtered_words)\n",
    "\n",
    "# def preprocess(name):\n",
    "#     name = remove_salutations(name)\n",
    "#     name = remove_parent_spouse_name(name)\n",
    "#     name = remove_common_muslim_variations(name)\n",
    "#     name = remove_agarwal_variants(name)\n",
    "#     name = convert_to_lower(name)\n",
    "#     name = replace_adjacent_duplicates(name)\n",
    "#     name = replace_characters(name)\n",
    "#     name = replace_bigrams(name)\n",
    "#     name = remove_consonant_a(name)\n",
    "#     name = remove_special_characters(name)\n",
    "#     name = remove_extra_spaces(name)\n",
    "#     name = remove_stop_words(name)\n",
    "#     return name\n",
    "\n",
    "\n",
    "# import pickle\n",
    "# with open('model.pkl', 'rb') as file:\n",
    "#     model = pickle.load(file)\n",
    "    \n",
    "# with open('tokenizer.pkl', 'rb') as file:\n",
    "#     tokenizer = pickle.load(file)\n",
    "    \n",
    "\n",
    "# def get_embedding(text):\n",
    "#     inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "#     with torch.no_grad():\n",
    "#         embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "# # Similarity functions\n",
    "# def calculate_cosine_similarity(embedding1, embedding2):\n",
    "#     return cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "# def calculate_levenshtein_similarity(name1, name2):\n",
    "#     lev_distance = levenshtein_distance(name1, name2)\n",
    "#     max_len = max(len(name1), len(name2))\n",
    "#     return (max_len - lev_distance) / max_len if max_len > 0 else 1.0\n",
    "\n",
    "# def calculate_phonetic_similarity(name1, name2):\n",
    "#     soundex1 = jellyfish.soundex(name1)\n",
    "#     soundex2 = jellyfish.soundex(name2)\n",
    "#     return jellyfish.jaro_winkler_similarity(soundex1, soundex2)\n",
    "\n",
    "# def calculate_jaccard_similarity(name1, name2):\n",
    "#     set1, set2 = set(name1), set(name2)\n",
    "#     intersection, union = set1.intersection(set2), set1.union(set2)\n",
    "#     return len(intersection) / len(union) if union else 1.0\n",
    "\n",
    "# ##------------------------------------Calling Name_Match------------------------------------------------------------------------------------------------------\n",
    " \n",
    "\n",
    "# def name_match(name1, name2):\n",
    "#     name1_processed = preprocess(name1)\n",
    "#     name2_processed = preprocess(name2)\n",
    "\n",
    "#     embedding_similarity = predict_similarity_embedding_model([name1_processed, name2_processed])\n",
    "\n",
    "#     levenshtein_similarity = calculate_levenshtein_similarity(name1_processed, name2_processed)\n",
    "#     phonetic_similarity = calculate_phonetic_similarity(name1_processed, name2_processed)\n",
    "#     jaccard_similarity = calculate_jaccard_similarity(name1_processed, name2_processed)\n",
    "\n",
    "#     fuzzy_ratio = fuzz.ratio(name1_processed, name2_processed) / 100.0\n",
    "#     fuzzy_partial_ratio = fuzz.partial_ratio(name1_processed, name2_processed) / 100.0\n",
    "#     fuzzy_token_sort_ratio = fuzz.token_sort_ratio(name1_processed, name2_processed) / 100.0\n",
    "#     fuzzy_token_set_ratio = fuzz.token_set_ratio(name1_processed, name2_processed) / 100.0\n",
    "\n",
    "#     fuzzy_similarity = (fuzzy_ratio + fuzzy_partial_ratio + fuzzy_token_sort_ratio + fuzzy_token_set_ratio) / 4.0\n",
    "    \n",
    "    \n",
    "# #     return {\n",
    "# #         \"embedding_similarity\": embedding_similarity,\n",
    "# #         \"levenshtein_similarity\": levenshtein_similarity,\n",
    "# #         \"phonetic_similarity\": phonetic_similarity,\n",
    "# #         \"jaccard_similarity\": jaccard_similarity,\n",
    "# #         \"fuzzy_similarity\": fuzzy_similarity\n",
    "# #     }\n",
    "  \n",
    "\n",
    "#     final_score = (\n",
    "#         embedding_similarity * 0.121630 +\n",
    "#         levenshtein_similarity * 0.205104 +\n",
    "#         phonetic_similarity * 0.022287 +\n",
    "#         jaccard_similarity * 0.262775 +\n",
    "#         fuzzy_similarity * 0.388205\n",
    "#     )\n",
    "#     return final_score\n",
    "\n",
    "\n",
    "# ##------------------------------------Fuzzy With Data Preprocessing and keyword matching-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# SPECIAL_CHAR_DOT_REGEX = r\"[.]\"\n",
    "# SPECIAL_CHARS_REGEX = r\"[-+.^:,_/\\s]+\" \n",
    "# SALUTATION_REGEX = r\"^(shree|shri|miss|smt|mrs|mr|ms|dr|master|hon|sir|madam|prof|capt|major|rev|fr|br)\\s*\"\n",
    "# PARENT_SPOUSE_NAME_REGEX = r\"(?:\\s*(?:s/o|d/o|w/o|so|do|wo|daughter of|son of|wife of|husband of)\\s*)\"\n",
    "# COMMON_MUSLIM_SALUTATIONS_MOHAMMAD_REGEX = r\"\\b(mohammad|mohammed|muhamed|mohd|mohamed|mohamad|muhamad|muhammad|muhammed|muhammet|mohamud|mohummad|mohummed|mouhamed|muhamaad|mohammod|mouhamad|mo|md|mahmood|mahmud|ahmad|ahmed|hameed|hamid|hammed|mahd|mahmod|mohd|mouhammed|mohamad|muhmood|mohhammed|muhmamed|mohmed|mohmat|muhmat|mu|m|shaikh|mo)\\b\"\n",
    "# LAST_NAMES_AGARWAL_VARIANTS_REGEX = r\"\\b(aggarwal|agrawal|agarwal|aggrawal|agarwalla|agarwal)\\b\"\n",
    "\n",
    "\n",
    "# def convert_to_lower(name):\n",
    "#     return name.lower()\n",
    "\n",
    "# def remove_extra_spaces(name):\n",
    "#     return re.sub(r'\\s+', ' ', name).strip()\n",
    "\n",
    "# def remove_common_muslim_variations(text):\n",
    "#     return re.sub(COMMON_MUSLIM_SALUTATIONS_MOHAMMAD_REGEX, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# def remove_agarwal_variants(text):\n",
    "#     return re.sub(LAST_NAMES_AGARWAL_VARIANTS_REGEX, '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# def remove_stop_words(text):\n",
    "#     # stop_words = ['devi', 'dei', 'debi', 'kumar', 'kumaar', 'kumari', 'kumaari', 'kmr', 'kumr', 'bhai', 'bhau', 'bai', 'ben', 'singh', 'kaur', 'Md', 'Mohd', 'Mohammad', 'Mohamad','alam','shekh','sek']\n",
    "#     stop_words = ['devi', 'dei', 'debi', 'kmr', 'kumr', 'bhai', 'bhau', 'bai', 'ben', 'kaur', 'Md', 'Mohd', 'Mohammad', 'Mohamad','alam','shekh','sek']\n",
    "#     words = text.split()\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "# def preprocess_FuzzyWuzzy(name):\n",
    "#     \"\"\"Process a name through all the defined normalization steps.\"\"\"\n",
    "#     name = convert_to_lower(name)\n",
    "#     name = remove_stop_words(name)\n",
    "#     name = remove_common_muslim_variations(name)\n",
    "#     name = remove_agarwal_variants(name)\n",
    "#     return name\n",
    "\n",
    "# def calculate_fuzzy_similarity_processed(name1, name2):\n",
    "#     name1 = preprocess_FuzzyWuzzy(name1)\n",
    "#     name2 = preprocess_FuzzyWuzzy(name2)\n",
    "    \n",
    "#     fuzzy_ratio = fuzz.ratio(name1, name2) / 100.0\n",
    "#     fuzzy_partial_ratio = fuzz.partial_ratio(name1, name2) / 100.0\n",
    "#     fuzzy_token_sort_ratio = fuzz.token_sort_ratio(name1, name2) / 100.0\n",
    "#     fuzzy_token_set_ratio = fuzz.token_set_ratio(name1, name2) / 100.0\n",
    "\n",
    "#     fuzzy_similarity = (fuzzy_ratio + fuzzy_partial_ratio + fuzzy_token_sort_ratio + fuzzy_token_set_ratio) / 4.0\n",
    "#     return fuzzy_similarity\n",
    "\n",
    "\n",
    "# def check_keywords_in_names(name1, name2):\n",
    "#     \"\"\"Check for presence of keywords in either or both names.\"\"\"\n",
    "#     found_in_name1 = any(keyword in name1.lower() for keyword in KEYWORDS)\n",
    "#     found_in_name2 = any(keyword in name2.lower() for keyword in KEYWORDS)\n",
    "    \n",
    "#     if found_in_name1 and found_in_name2:\n",
    "#         return 1  \n",
    "#     elif found_in_name1 or found_in_name2:\n",
    "#         return 0  \n",
    "#     return 1\n",
    "\n",
    "\n",
    "# def process_false_cases(row):\n",
    "#     name1 = row['name1']\n",
    "#     name2 = row['name2']\n",
    "\n",
    "#     fuzzy_SS = calculate_fuzzy_similarity_processed(name1, name2)\n",
    "\n",
    "#     fuzzy_flag = 1 if fuzzy_SS >= 0.70 else 0\n",
    "\n",
    "#     if fuzzy_flag:\n",
    "#         fuzzy_flag = check_keywords_in_names(name1, name2)\n",
    "\n",
    "#     return 1 if fuzzy_flag else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cce4ac-af07-4cde-b3a1-8868745ac0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839e6e3-27c9-49fa-8e83-cc4d1b1e5f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679967e1-3536-45fb-beb6-d8fd38d7cd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29376f72-7de7-4e1b-b393-bdb263a998d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5529b-43bb-42d6-a978-ea8f4178280e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73398e3f-71f0-4de6-b0a3-e18f294ed1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123a52e-3a29-484a-871b-9fa34d1fa720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b651ac-fd52-44af-b0bf-75c83a24121d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fb814a8-8993-4a4b-ad8e-8c2561e6f789",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_1884116/1835758383.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
      "/var/tmp/ipykernel_1884116/1835758383.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
      "/var/tmp/ipykernel_1884116/1835758383.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"].astype(int)\n",
      "/var/tmp/ipykernel_1884116/1835758383.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
      "/var/tmp/ipykernel_1884116/1835758383.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Each Layer:\n",
      "\n",
      "First Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5175            4\n",
      "Actual 1          559         4262\n",
      "Accuracy: 0.9437\n",
      "Precision: 0.9990623534927332\n",
      "Recall: 0.8840489524994815\n",
      "F1 Score: 0.9380433586442171\n",
      "AUC: 0.9416383013124942\n",
      "\n",
      "Second Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5169            6\n",
      "Actual 1          177          382\n",
      "Accuracy: 0.9680851063829787\n",
      "Precision: 0.9845360824742269\n",
      "Recall: 0.6833631484794276\n",
      "F1 Score: 0.8067581837381204\n",
      "AUC: 0.8411018640947863\n",
      "\n",
      "Third Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5131           38\n",
      "Actual 1          126           51\n",
      "Accuracy: 0.9693228582117471\n",
      "Precision: 0.5730337078651685\n",
      "Recall: 0.288135593220339\n",
      "F1 Score: 0.38345864661654133\n",
      "AUC: 0.6403920372756754\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    y_true = df['labels']  # True labels\n",
    "    y_pred = df['Prediction']  # Predicted labels\n",
    "    y_scores = df['Prediction'].astype(float)  # Convert integers to float for AUC calculation\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "    # Load data for first layer\n",
    "    df_layer1 = pd.read_csv(file_path)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    df_layer1[\"Prediction\"] = 0  \n",
    "\n",
    "    # First Layer\n",
    "    df_layer1[\"First_Layer_Pass\"] = df_layer1.apply(fuzzy_layer1, axis=1)\n",
    "    df_layer1[\"Prediction\"] = df_layer1.apply(lambda x: 1 if x[\"First_Layer_Pass\"] else x[\"Prediction\"], axis=1)\n",
    "    \n",
    "    # Save first layer result\n",
    "    df_layer1.to_csv(\"first_audit.csv\", index=False)\n",
    "    first_layer_metrics = calculate_metrics(df_layer1)\n",
    "\n",
    "    # Second Layer\n",
    "    df_layer2 = df_layer1[df_layer1[\"Prediction\"] == 0]  # Filter rows where Prediction == 0\n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "    df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"].astype(int)\n",
    "    \n",
    "    # Save second layer result\n",
    "    df_layer2.to_csv(\"second_audit.csv\", index=False)\n",
    "    second_layer_metrics = calculate_metrics(df_layer2)\n",
    "\n",
    "    # Third Layer\n",
    "    df_layer3 = df_layer2[df_layer2[\"Prediction\"] == 0]  # Filter rows where Prediction == 0\n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "    df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"].astype(int)\n",
    "    \n",
    "    # Save third layer result\n",
    "    df_layer3.to_csv(\"third_audit.csv\", index=False)\n",
    "    third_layer_metrics = calculate_metrics(df_layer3)\n",
    "    \n",
    "    # Combine results\n",
    "    df_combined = pd.concat([\n",
    "        df_layer1[df_layer1[\"Prediction\"] == 1],\n",
    "        df_layer2[df_layer2[\"Prediction\"] == 1],\n",
    "        df_layer3[df_layer3[\"Prediction\"] == 1]\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # Save combined result to a CSV file\n",
    "    df_combined.to_csv(\"combined_results.csv\", index=False)\n",
    "\n",
    "    # Return results\n",
    "    return first_layer_metrics, second_layer_metrics, third_layer_metrics, df_combined\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    first_metrics, second_metrics, third_metrics, combined_data = process_name_matching(\"one_week.csv\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for Each Layer:\")\n",
    "\n",
    "    # First Layer Confusion Matrix\n",
    "    print(\"\\nFirst Layer Confusion Matrix:\")\n",
    "    first_cm = first_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(first_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {first_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {first_metrics['precision']}\")\n",
    "    print(f\"Recall: {first_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {first_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {first_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Second Layer Confusion Matrix\n",
    "    print(\"\\nSecond Layer Confusion Matrix:\")\n",
    "    second_cm = second_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(second_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {second_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {second_metrics['precision']}\")\n",
    "    print(f\"Recall: {second_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {second_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {second_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Third Layer Confusion Matrix\n",
    "    print(\"\\nThird Layer Confusion Matrix:\")\n",
    "    third_cm = third_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(third_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {third_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {third_metrics['precision']}\")\n",
    "    print(f\"Recall: {third_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {third_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {third_metrics['roc_auc_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb79397-b36b-48ce-a425-98f0c222fb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59da45-3419-4761-8a70-14a088336dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a6237-c7c4-46ad-a2c5-a607ae7e85d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021714cc-4e43-4546-b2e5-60e41a413f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5bffc31-8471-4526-8a6d-1c9570ef0de8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## close "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dba6e9-7ece-43dc-89a7-e636a7cc8b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "222501e0-2eb3-47a9-85eb-1ff08bd6ebd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_combined\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_combined' is not defined"
     ]
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22080260-a214-4099-a16d-12949f3f3951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined['Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94f745-1812-4034-9ff0-20abdf4a0bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d9ab6-ae46-4644-8d59-7ea98df967b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdf3c71e-946c-415a-b182-77a21a2ae7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210055, 17)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e909afbd-ef5c-40e2-974d-e9858a3f9c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction\n",
       "True     114504\n",
       "False     95551\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50873de6-b872-4d40-8562-d3ad0851316b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89be1fc2-8ea1-428f-bf36-898a67927ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_686919/891376527.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
      "/var/tmp/ipykernel_686919/891376527.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
      "/var/tmp/ipykernel_686919/891376527.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Each Layer:\n",
      "\n",
      "First Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5178            1\n",
      "Actual 1         1089         3732\n",
      "Accuracy: 0.891\n",
      "Precision: 0.999732118939191\n",
      "Recall: 0.7741132545115121\n",
      "F1 Score: 0.8725742342763619\n",
      "AUC: 0.8869600835214445\n",
      "\n",
      "Second Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5177            1\n",
      "Actual 1          339          750\n",
      "Accuracy: 0.9457475666187969\n",
      "Precision: 0.9986684420772304\n",
      "Recall: 0.6887052341597796\n",
      "F1 Score: 0.8152173913043478\n",
      "AUC: 0.844256054700593\n",
      "\n",
      "Third Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5177            0\n",
      "Actual 1          332            7\n",
      "Accuracy: 0.939811457577955\n",
      "Precision: 1.0\n",
      "Recall: 0.02064896755162242\n",
      "F1 Score: 0.04046242774566474\n",
      "AUC: 0.5103244837758112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_686919/891376527.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
      "/var/tmp/ipykernel_686919/891376527.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    y_true = df['labels']  # True labels\n",
    "    y_pred = df['Prediction']  # Predicted labels\n",
    "    y_scores = df['Prediction'].astype(float)  # Convert boolean to float for AUC calculation\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "    # Load data for first layer\n",
    "    df_layer1 = pd.read_csv(file_path)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    # First Layer\n",
    "    df[\"First_Layer_Pass\"] = df.apply(fuzzy_layer1, axis=1)\n",
    "    df[\"Prediction\"] = df.apply(lambda x: 1 if x[\"First_Layer_Pass\"] else x[\"Prediction\"], axis=1)\n",
    "    \n",
    "    # Save first layer result\n",
    "    df_layer1.to_csv(\"first_audit.csv\", index=False)\n",
    "    first_layer_metrics = calculate_metrics(df_layer1)\n",
    "\n",
    "    # Second Layer\n",
    "    df_layer2 = df_layer1[df_layer1[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "    df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n",
    "    \n",
    "    # Save second layer result\n",
    "    df_layer2.to_csv(\"second_audit.csv\", index=False)\n",
    "    second_layer_metrics = calculate_metrics(df_layer2)\n",
    "\n",
    "    # Third Layer\n",
    "    df_layer3 = df_layer2[df_layer2[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "    df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n",
    "    \n",
    "    # Save third layer result\n",
    "    df_layer3.to_csv(\"third_audit.csv\", index=False)\n",
    "    third_layer_metrics = calculate_metrics(df_layer3)\n",
    "    \n",
    "    # Combine results\n",
    "    df_combined = pd.concat([\n",
    "        df_layer1[df_layer1[\"Prediction\"]],\n",
    "        df_layer2[df_layer2[\"Prediction\"]],\n",
    "        df_layer3[df_layer3[\"Prediction\"]]\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # Save combined result to a CSV file\n",
    "    df_combined.to_csv(\"combined_results.csv\", index=False)\n",
    "\n",
    "    # Return results\n",
    "    return first_layer_metrics, second_layer_metrics, third_layer_metrics, df_combined\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    first_metrics, second_metrics, third_metrics, combined_data = process_name_matching(\"combined_data_10k.csv\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for Each Layer:\")\n",
    "\n",
    "    # First Layer Confusion Matrix\n",
    "    print(\"\\nFirst Layer Confusion Matrix:\")\n",
    "    first_cm = first_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(first_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {first_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {first_metrics['precision']}\")\n",
    "    print(f\"Recall: {first_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {first_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {first_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Second Layer Confusion Matrix\n",
    "    print(\"\\nSecond Layer Confusion Matrix:\")\n",
    "    second_cm = second_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(second_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {second_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {second_metrics['precision']}\")\n",
    "    print(f\"Recall: {second_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {second_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {second_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Third Layer Confusion Matrix\n",
    "    print(\"\\nThird Layer Confusion Matrix:\")\n",
    "    third_cm = third_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(third_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {third_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {third_metrics['precision']}\")\n",
    "    print(f\"Recall: {third_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {third_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {third_metrics['roc_auc_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114b02ef-c82a-4b14-84a3-e28d632035c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_686919/891376527.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
      "/var/tmp/ipykernel_686919/891376527.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
      "/var/tmp/ipykernel_686919/891376527.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Each Layer:\n",
      "\n",
      "First Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5178            1\n",
      "Actual 1         1089         3732\n",
      "Accuracy: 0.891\n",
      "Precision: 0.999732118939191\n",
      "Recall: 0.7741132545115121\n",
      "F1 Score: 0.8725742342763619\n",
      "AUC: 0.8869600835214445\n",
      "\n",
      "Second Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5177            1\n",
      "Actual 1          339          750\n",
      "Accuracy: 0.9457475666187969\n",
      "Precision: 0.9986684420772304\n",
      "Recall: 0.6887052341597796\n",
      "F1 Score: 0.8152173913043478\n",
      "AUC: 0.844256054700593\n",
      "\n",
      "Third Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5164           13\n",
      "Actual 1          221          118\n",
      "Accuracy: 0.957577955039884\n",
      "Precision: 0.9007633587786259\n",
      "Recall: 0.3480825958702065\n",
      "F1 Score: 0.5021276595744681\n",
      "AUC: 0.6727857445257929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_686919/891376527.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
      "/var/tmp/ipykernel_686919/891376527.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    y_true = df['labels']  # True labels\n",
    "    y_pred = df['Prediction']  # Predicted labels\n",
    "    y_scores = df['Prediction'].astype(float)  # Convert boolean to float for AUC calculation\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "    # Load data for first layer\n",
    "    df_layer1 = pd.read_csv(file_path)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    # First Layer\n",
    "    df_layer1[\"First_Layer_Score\"] = df_layer1.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df_layer1[\"First_Layer_Pass\"] = df_layer1[\"First_Layer_Score\"] >= threshold\n",
    "    df_layer1[\"Prediction\"] = df_layer1[\"First_Layer_Pass\"]\n",
    "    \n",
    "    # Save first layer result\n",
    "    df_layer1.to_csv(\"first_audit.csv\", index=False)\n",
    "    first_layer_metrics = calculate_metrics(df_layer1)\n",
    "\n",
    "    # Second Layer\n",
    "    df_layer2 = df_layer1[df_layer1[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "    df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n",
    "    \n",
    "    # Save second layer result\n",
    "    df_layer2.to_csv(\"second_audit.csv\", index=False)\n",
    "    second_layer_metrics = calculate_metrics(df_layer2)\n",
    "\n",
    "    # Third Layer\n",
    "    df_layer3 = df_layer2[df_layer2[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "    df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n",
    "    \n",
    "    # Save third layer result\n",
    "    df_layer3.to_csv(\"third_audit.csv\", index=False)\n",
    "    third_layer_metrics = calculate_metrics(df_layer3)\n",
    "    \n",
    "    # Combine results\n",
    "    df_combined = pd.concat([\n",
    "        df_layer1[df_layer1[\"Prediction\"]],\n",
    "        df_layer2[df_layer2[\"Prediction\"]],\n",
    "        df_layer3[df_layer3[\"Prediction\"]]\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # Save combined result to a CSV file\n",
    "    df_combined.to_csv(\"combined_results.csv\", index=False)\n",
    "\n",
    "    # Return results\n",
    "    return first_layer_metrics, second_layer_metrics, third_layer_metrics, df_combined\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    first_metrics, second_metrics, third_metrics, combined_data = process_name_matching(\"combined_data_10k.csv\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for Each Layer:\")\n",
    "\n",
    "    # First Layer Confusion Matrix\n",
    "    print(\"\\nFirst Layer Confusion Matrix:\")\n",
    "    first_cm = first_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(first_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {first_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {first_metrics['precision']}\")\n",
    "    print(f\"Recall: {first_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {first_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {first_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Second Layer Confusion Matrix\n",
    "    print(\"\\nSecond Layer Confusion Matrix:\")\n",
    "    second_cm = second_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(second_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {second_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {second_metrics['precision']}\")\n",
    "    print(f\"Recall: {second_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {second_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {second_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Third Layer Confusion Matrix\n",
    "    print(\"\\nThird Layer Confusion Matrix:\")\n",
    "    third_cm = third_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(third_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {third_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {third_metrics['precision']}\")\n",
    "    print(f\"Recall: {third_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {third_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {third_metrics['roc_auc_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dffb3d3a-8815-486e-8664-b0444efb69cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_767108/1989567890.py:1: DtypeWarning: Columns (15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(\"combined_results_5.csv\")\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"combined_results_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4f59174-b3ff-4ac2-9598-25ee602c98c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"fined_data_audit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04b57ae8-e72e-42db-b483-34b477151a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134826, 11)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49370f-7f7c-441b-8ec0-64536fa2ff65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3aac6-51ed-481a-8b53-5e4d46b2050a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d919f-01dd-4f83-9044-be34ac6854f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60ada10d-2307-4742-ab6b-2b84ca052cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction\n",
       "True    116944\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e081e77-8a55-4fc0-8d26-21d12526dfa5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "562d4d29-088c-4452-aa89-a3c0cb510179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2555386/2302088866.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
      "/var/tmp/ipykernel_2555386/2302088866.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
      "/var/tmp/ipykernel_2555386/2302088866.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Each Layer:\n",
      "\n",
      "First Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6998          100\n",
      "Actual 1         2661         5339\n",
      "Accuracy: 0.8171280964366141\n",
      "Precision: 0.9816142673285531\n",
      "Recall: 0.667375\n",
      "F1 Score: 0.7945531661581963\n",
      "AUC: 0.8266432621865315\n",
      "\n",
      "Second Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6892          106\n",
      "Actual 1          834         1827\n",
      "Accuracy: 0.9026814370017601\n",
      "Precision: 0.9451629591308847\n",
      "Recall: 0.6865839909808342\n",
      "F1 Score: 0.7953852851545494\n",
      "AUC: 0.83571840303543\n",
      "\n",
      "Third Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6880           12\n",
      "Actual 1          629          205\n",
      "Accuracy: 0.9170333937354388\n",
      "Precision: 0.9447004608294931\n",
      "Recall: 0.24580335731414868\n",
      "F1 Score: 0.39010466222645096\n",
      "AUC: 0.622031104077852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2555386/2302088866.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
      "/var/tmp/ipykernel_2555386/2302088866.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    y_true = df['labels']  # True labels\n",
    "    y_pred = df['Prediction']  # Predicted labels\n",
    "    y_scores = df['Prediction'].astype(float)  # Convert boolean to float for AUC calculation\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "    # Load data for first layer\n",
    "    df_layer1 = pd.read_csv(file_path)\n",
    "    threshold = 0.75\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    # First Layer\n",
    "    df_layer1[\"First_Layer_Score\"] = df_layer1.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df_layer1[\"First_Layer_Pass\"] = df_layer1[\"First_Layer_Score\"] >= threshold\n",
    "    df_layer1[\"Prediction\"] = df_layer1[\"First_Layer_Pass\"]\n",
    "    \n",
    "    # Save first layer result\n",
    "    df_layer1.to_csv(\"first_audit.csv\", index=False)\n",
    "    first_layer_metrics = calculate_metrics(df_layer1)\n",
    "\n",
    "    # Second Layer\n",
    "    df_layer2 = df_layer1[df_layer1[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "    df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n",
    "    \n",
    "    # Save second layer result\n",
    "    df_layer2.to_csv(\"second_audit.csv\", index=False)\n",
    "    second_layer_metrics = calculate_metrics(df_layer2)\n",
    "\n",
    "    # Third Layer\n",
    "    df_layer3 = df_layer2[df_layer2[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "    df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n",
    "    \n",
    "    # Save third layer result\n",
    "    df_layer3.to_csv(\"third_audit.csv\", index=False)\n",
    "    third_layer_metrics = calculate_metrics(df_layer3)\n",
    "    \n",
    "    # Combine results\n",
    "    df_combined = pd.concat([\n",
    "        df_layer1[df_layer1[\"Prediction\"]],\n",
    "        df_layer2[df_layer2[\"Prediction\"]],\n",
    "        df_layer3[df_layer3[\"Prediction\"]]\n",
    "    ])\n",
    "\n",
    "    # Final output\n",
    "    df_layer1[\"Prediction\"] = False\n",
    "    df_layer1.loc[df_layer1[df_layer1[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df_layer1.loc[df_layer2[df_layer2[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df_layer1.loc[df_layer3[df_layer3[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "\n",
    "    return first_layer_metrics, second_layer_metrics, third_layer_metrics, df_combined\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    first_metrics, second_metrics, third_metrics, combined_data = process_name_matching(\"balanced_audit_data.csv\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for Each Layer:\")\n",
    "\n",
    "    # First Layer Confusion Matrix\n",
    "    print(\"\\nFirst Layer Confusion Matrix:\")\n",
    "    first_cm = first_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(first_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {first_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {first_metrics['precision']}\")\n",
    "    print(f\"Recall: {first_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {first_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {first_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Second Layer Confusion Matrix\n",
    "    print(\"\\nSecond Layer Confusion Matrix:\")\n",
    "    second_cm = second_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(second_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {second_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {second_metrics['precision']}\")\n",
    "    print(f\"Recall: {second_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {second_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {second_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Third Layer Confusion Matrix\n",
    "    print(\"\\nThird Layer Confusion Matrix:\")\n",
    "    third_cm = third_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(third_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {third_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {third_metrics['precision']}\")\n",
    "    print(f\"Recall: {third_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {third_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {third_metrics['roc_auc_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a7627-5a64-4cac-ac89-375e16591462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99eb5ba4-47ec-4654-b1a0-53bc3c263ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2555386/2513829123.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
      "/var/tmp/ipykernel_2555386/2513829123.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
      "/var/tmp/ipykernel_2555386/2513829123.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Each Layer:\n",
      "\n",
      "First Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         7054           44\n",
      "Actual 1         3062         4938\n",
      "Accuracy: 0.7942773877334747\n",
      "Precision: 0.9911682055399438\n",
      "Recall: 0.61725\n",
      "F1 Score: 0.7607456478200586\n",
      "AUC: 0.8055255353620738\n",
      "\n",
      "Second Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6908          146\n",
      "Actual 1         1028         2034\n",
      "Accuracy: 0.883946223803875\n",
      "Precision: 0.9330275229357798\n",
      "Recall: 0.6642717178314826\n",
      "F1 Score: 0.7760396795116368\n",
      "AUC: 0.8217871206112333\n",
      "\n",
      "Third Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         6884           24\n",
      "Actual 1          653          375\n",
      "Accuracy: 0.9146925403225806\n",
      "Precision: 0.9398496240601504\n",
      "Recall: 0.3647859922178988\n",
      "F1 Score: 0.5255781359495444\n",
      "AUC: 0.6806558797221515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2555386/2513829123.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
      "/var/tmp/ipykernel_2555386/2513829123.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    y_true = df['labels']  # True labels\n",
    "    y_pred = df['Prediction']  # Predicted labels\n",
    "    y_scores = df['Prediction'].astype(float)  # Convert boolean to float for AUC calculation\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "    # Load data for first layer\n",
    "    df_layer1 = pd.read_csv(file_path)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    # First Layer\n",
    "    df_layer1[\"First_Layer_Score\"] = df_layer1.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df_layer1[\"First_Layer_Pass\"] = df_layer1[\"First_Layer_Score\"] >= threshold\n",
    "    df_layer1[\"Prediction\"] = df_layer1[\"First_Layer_Pass\"]\n",
    "    \n",
    "    # Save first layer result\n",
    "    df_layer1.to_csv(\"first_audit.csv\", index=False)\n",
    "    first_layer_metrics = calculate_metrics(df_layer1)\n",
    "\n",
    "    # Second Layer\n",
    "    df_layer2 = df_layer1[df_layer1[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "    df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n",
    "    \n",
    "    # Save second layer result\n",
    "    df_layer2.to_csv(\"second_audit.csv\", index=False)\n",
    "    second_layer_metrics = calculate_metrics(df_layer2)\n",
    "\n",
    "    # Third Layer\n",
    "    df_layer3 = df_layer2[df_layer2[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "    df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n",
    "    \n",
    "    # Save third layer result\n",
    "    df_layer3.to_csv(\"third_audit.csv\", index=False)\n",
    "    third_layer_metrics = calculate_metrics(df_layer3)\n",
    "    \n",
    "    # Combine results\n",
    "    df_combined = pd.concat([\n",
    "        df_layer1[df_layer1[\"Prediction\"]],\n",
    "        df_layer2[df_layer2[\"Prediction\"]],\n",
    "        df_layer3[df_layer3[\"Prediction\"]]\n",
    "    ])\n",
    "\n",
    "    # Final output\n",
    "    df_layer1[\"Prediction\"] = False\n",
    "    df_layer1.loc[df_layer1[df_layer1[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df_layer1.loc[df_layer2[df_layer2[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df_layer1.loc[df_layer3[df_layer3[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "\n",
    "    return first_layer_metrics, second_layer_metrics, third_layer_metrics, df_combined\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    first_metrics, second_metrics, third_metrics, combined_data = process_name_matching(\"balanced_audit_data.csv\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for Each Layer:\")\n",
    "\n",
    "    # First Layer Confusion Matrix\n",
    "    print(\"\\nFirst Layer Confusion Matrix:\")\n",
    "    first_cm = first_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(first_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {first_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {first_metrics['precision']}\")\n",
    "    print(f\"Recall: {first_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {first_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {first_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Second Layer Confusion Matrix\n",
    "    print(\"\\nSecond Layer Confusion Matrix:\")\n",
    "    second_cm = second_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(second_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {second_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {second_metrics['precision']}\")\n",
    "    print(f\"Recall: {second_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {second_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {second_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Third Layer Confusion Matrix\n",
    "    print(\"\\nThird Layer Confusion Matrix:\")\n",
    "    third_cm = third_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(third_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {third_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {third_metrics['precision']}\")\n",
    "    print(f\"Recall: {third_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {third_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {third_metrics['roc_auc_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033680c-8132-47c5-b85f-9101f1a4d79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5dbe6-eb40-4a10-b04d-0a2baf724077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f327c39-4220-4623-95ec-1796acf13843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5ccef-52ee-488d-aebf-a8db150dbfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0180d959-26b9-42ee-8428-7026cb7f6bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics Summary:\n",
      "\n",
      "Threshold: 0.6\n",
      "Accuracy: 0.9173739147400254\n",
      "Precision: 0.956370534302877\n",
      "Recall: 0.8763882182520522\n",
      "F1 Score: 0.9146341463414634\n",
      "AUC: 0.917793990864471\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         9733          414\n",
      "Actual 1         1280         9075\n",
      "\n",
      "Threshold: 0.65\n",
      "Accuracy: 0.8788898644034728\n",
      "Precision: 0.9758220502901354\n",
      "Recall: 0.7795267986479961\n",
      "F1 Score: 0.8666988779728351\n",
      "AUC: 0.8799082697290439\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         9947          200\n",
      "Actual 1         2283         8072\n",
      "\n",
      "Threshold: 0.7\n",
      "Accuracy: 0.8651351087698761\n",
      "Precision: 0.9896774193548387\n",
      "Recall: 0.7407049734427813\n",
      "F1 Score: 0.8472797569732119\n",
      "AUC: 0.866410434883409\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10067           80\n",
      "Actual 1         2685         7670\n",
      "\n",
      "Threshold: 0.75\n",
      "Accuracy: 0.848990342405619\n",
      "Precision: 0.997805513646962\n",
      "Recall: 0.7025591501690005\n",
      "F1 Score: 0.824549472968378\n",
      "AUC: 0.8504911647169039\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10131           16\n",
      "Actual 1         3080         7275\n",
      "\n",
      "Threshold: 0.8\n",
      "Accuracy: 0.8198712320749195\n",
      "Precision: 1.0\n",
      "Recall: 0.6433606953162724\n",
      "F1 Score: 0.7829817241581948\n",
      "AUC: 0.8216803476581362\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10147            0\n",
      "Actual 1         3693         6662\n",
      "\n",
      "Threshold: 0.85\n",
      "Accuracy: 0.7918251877865574\n",
      "Precision: 1.0\n",
      "Recall: 0.5878319652341863\n",
      "F1 Score: 0.7404208733730689\n",
      "AUC: 0.7939159826170932\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10147            0\n",
      "Actual 1         4268         6087\n",
      "\n",
      "Threshold: 0.9\n",
      "Accuracy: 0.7648522095405327\n",
      "Precision: 1.0\n",
      "Recall: 0.5344278126508933\n",
      "F1 Score: 0.6965825413808296\n",
      "AUC: 0.7672139063254466\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10147            0\n",
      "Actual 1         4821         5534\n",
      "\n",
      "Threshold: 0.95\n",
      "Accuracy: 0.729880011706175\n",
      "Precision: 1.0\n",
      "Recall: 0.4651859005311444\n",
      "F1 Score: 0.6349854996045348\n",
      "AUC: 0.7325929502655721\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10147            0\n",
      "Actual 1         5538         4817\n",
      "\n",
      "Threshold: 1\n",
      "Accuracy: 0.6543751829089844\n",
      "Precision: 1.0\n",
      "Recall: 0.31569290197971994\n",
      "F1 Score: 0.4798884321785085\n",
      "AUC: 0.6578464509898599\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10147            0\n",
      "Actual 1         7086         3269\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    y_true = df['labels']\n",
    "    y_pred = df['Prediction']\n",
    "    y_scores = df['Prediction'].astype(float)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "def process_name_matching(file_path, thresholds, output_dir=\"output\"):\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    results = []\n",
    "    combined_predictions = pd.DataFrame()\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Apply threshold\n",
    "        df[f\"Score_{threshold}\"] = df.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "        df[\"Prediction\"] = df[f\"Score_{threshold}\"] >= threshold\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(df)\n",
    "        metrics[\"threshold\"] = threshold\n",
    "        results.append(metrics)\n",
    "\n",
    "        # Save layer-wise data\n",
    "        df.to_csv(f\"{output_dir}/results_threshold_{int(threshold * 100)}.csv\", index=False)\n",
    "\n",
    "        # Combine passing rows for final prediction\n",
    "        combined_predictions = pd.concat([combined_predictions, df[df[\"Prediction\"]]])\n",
    "\n",
    "    # Save combined data\n",
    "    combined_predictions.to_csv(f\"{output_dir}/combined_predictions.csv\", index=False)\n",
    "\n",
    "    return results, combined_predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    thresholds = [0.60, 0.65, 0.70,0.75, 0.80, 0.85, 0.90, 0.95, 1]  # Define thresholds for evaluation\n",
    "    metrics, combined_data = process_name_matching(\"combined_data_20k.csv\", thresholds)\n",
    "\n",
    "    print(\"\\nMetrics Summary:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"\\nThreshold: {metric['threshold']}\")\n",
    "        print(f\"Accuracy: {metric['accuracy']}\")\n",
    "        print(f\"Precision: {metric['precision']}\")\n",
    "        print(f\"Recall: {metric['recall']}\")\n",
    "        print(f\"F1 Score: {metric['f1_score']}\")\n",
    "        print(f\"AUC: {metric['roc_auc_score']}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(pd.DataFrame(\n",
    "            metric[\"confusion_matrix\"],\n",
    "            index=[\"Actual 0\", \"Actual 1\"],\n",
    "            columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b7e50-0a5b-48e8-9aa2-e7700bf51a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391c904-2eeb-45f9-ade1-8e47ba8a7217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e211d8-3bd0-4640-a06d-241e3f087685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70f18e-b23f-4368-8266-d030285970bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcbf5ab5-4893-4224-b959-febbc2965704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2109400/3837540159.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
      "/var/tmp/ipykernel_2109400/3837540159.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
      "/var/tmp/ipykernel_2109400/3837540159.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Each Layer:\n",
      "\n",
      "First Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10147            0\n",
      "Actual 1         3693         6662\n",
      "Accuracy: 0.8198712320749195\n",
      "Precision: 1.0\n",
      "Recall: 0.6433606953162724\n",
      "F1 Score: 0.7829817241581948\n",
      "AUC: 0.8216803476581362\n",
      "\n",
      "Second Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10146            1\n",
      "Actual 1          807         2886\n",
      "Accuracy: 0.9416184971098266\n",
      "Precision: 0.9996536196744025\n",
      "Recall: 0.7814784727863525\n",
      "F1 Score: 0.8772036474164133\n",
      "AUC: 0.8906899607452016\n",
      "\n",
      "Third Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10146            0\n",
      "Actual 1          402          405\n",
      "Accuracy: 0.9632977266502328\n",
      "Precision: 1.0\n",
      "Recall: 0.5018587360594795\n",
      "F1 Score: 0.6683168316831682\n",
      "AUC: 0.7509293680297398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2109400/3837540159.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
      "/var/tmp/ipykernel_2109400/3837540159.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    y_true = df['labels']  # True labels\n",
    "    y_pred = df['Prediction']  # Predicted labels\n",
    "    y_scores = df['Prediction'].astype(float)  # Convert boolean to float for AUC calculation\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "    # Load data for first layer\n",
    "    df_layer1 = pd.read_csv(file_path)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    # First Layer\n",
    "    df_layer1[\"First_Layer_Score\"] = df_layer1.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df_layer1[\"First_Layer_Pass\"] = df_layer1[\"First_Layer_Score\"] >= threshold\n",
    "    df_layer1[\"Prediction\"] = df_layer1[\"First_Layer_Pass\"]\n",
    "    \n",
    "    # Save first layer result\n",
    "    df_layer1.to_csv(\"first_layer_results_20k.csv\", index=False)\n",
    "    first_layer_metrics = calculate_metrics(df_layer1)\n",
    "\n",
    "    # Second Layer\n",
    "    df_layer2 = df_layer1[df_layer1[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "    df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n",
    "    \n",
    "    # Save second layer result\n",
    "    df_layer2.to_csv(\"second_layer_results_20k.csv\", index=False)\n",
    "    second_layer_metrics = calculate_metrics(df_layer2)\n",
    "\n",
    "    # Third Layer\n",
    "    df_layer3 = df_layer2[df_layer2[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "    df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n",
    "    \n",
    "    # Save third layer result\n",
    "    df_layer3.to_csv(\"third_layer_results_20k.csv\", index=False)\n",
    "    third_layer_metrics = calculate_metrics(df_layer3)\n",
    "    \n",
    "    # Combine results\n",
    "    df_combined = pd.concat([\n",
    "        df_layer1[df_layer1[\"Prediction\"]],\n",
    "        df_layer2[df_layer2[\"Prediction\"]],\n",
    "        df_layer3[df_layer3[\"Prediction\"]]\n",
    "    ])\n",
    "\n",
    "    # Final output\n",
    "    df_layer1[\"Prediction\"] = False\n",
    "    df_layer1.loc[df_layer1[df_layer1[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df_layer1.loc[df_layer2[df_layer2[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df_layer1.loc[df_layer3[df_layer3[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "\n",
    "    return first_layer_metrics, second_layer_metrics, third_layer_metrics, df_combined\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    first_metrics, second_metrics, third_metrics, combined_data = process_name_matching(\"combined_data_20k.csv\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for Each Layer:\")\n",
    "\n",
    "    # First Layer Confusion Matrix\n",
    "    print(\"\\nFirst Layer Confusion Matrix:\")\n",
    "    first_cm = first_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(first_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {first_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {first_metrics['precision']}\")\n",
    "    print(f\"Recall: {first_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {first_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {first_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Second Layer Confusion Matrix\n",
    "    print(\"\\nSecond Layer Confusion Matrix:\")\n",
    "    second_cm = second_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(second_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {second_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {second_metrics['precision']}\")\n",
    "    print(f\"Recall: {second_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {second_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {second_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Third Layer Confusion Matrix\n",
    "    print(\"\\nThird Layer Confusion Matrix:\")\n",
    "    third_cm = third_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(third_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {third_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {third_metrics['precision']}\")\n",
    "    print(f\"Recall: {third_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {third_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {third_metrics['roc_auc_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2d523-cfbd-4e65-b07d-b3e5c000eacb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc87b41d-abcd-47c4-9103-00ea8ecd9139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_20k=pd.read_csv(\"combined_data_20k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c615667b-dfd2-466b-ac30-a5a4271a0543",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "1    10355\n",
       "0    10147\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_20k['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb235511-a921-476b-b97d-831c09a8340d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## each layer result along with the csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47dff688-99fb-47d0-b585-874aeb46efbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2109400/3106890833.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
      "/var/tmp/ipykernel_2109400/3106890833.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
      "/var/tmp/ipykernel_2109400/3106890833.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Each Layer:\n",
      "\n",
      "First Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5178            1\n",
      "Actual 1         1998         2823\n",
      "Accuracy: 0.8001\n",
      "Precision: 0.9996458923512748\n",
      "Recall: 0.5855631611698817\n",
      "F1 Score: 0.7385219097449313\n",
      "AUC: 0.7926850368506292\n",
      "\n",
      "Second Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5177            1\n",
      "Actual 1          372         1626\n",
      "Accuracy: 0.9480211817168339\n",
      "Precision: 0.9993853718500307\n",
      "Recall: 0.8138138138138138\n",
      "F1 Score: 0.8971034482758621\n",
      "AUC: 0.9068103445276099\n",
      "\n",
      "Third Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5163           14\n",
      "Actual 1          209          163\n",
      "Accuracy: 0.9598125788430348\n",
      "Precision: 0.9209039548022598\n",
      "Recall: 0.4381720430107527\n",
      "F1 Score: 0.5938069216757741\n",
      "AUC: 0.7177338870645805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2109400/3106890833.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
      "/var/tmp/ipykernel_2109400/3106890833.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    y_true = df['labels']  # True labels\n",
    "    y_pred = df['Prediction']  # Predicted labels\n",
    "    y_scores = df['Prediction'].astype(float)  # Convert boolean to float for AUC calculation\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "    # Load data for first layer\n",
    "    df_layer1 = pd.read_csv(file_path)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    # First Layer\n",
    "    df_layer1[\"First_Layer_Score\"] = df_layer1.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df_layer1[\"First_Layer_Pass\"] = df_layer1[\"First_Layer_Score\"] >= threshold\n",
    "    df_layer1[\"Prediction\"] = df_layer1[\"First_Layer_Pass\"]\n",
    "    \n",
    "    # Save first layer result\n",
    "    df_layer1.to_csv(\"first_layer_results_10k.csv\", index=False)\n",
    "    first_layer_metrics = calculate_metrics(df_layer1)\n",
    "\n",
    "    # Second Layer\n",
    "    df_layer2 = df_layer1[df_layer1[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "    df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n",
    "    \n",
    "    # Save second layer result\n",
    "    df_layer2.to_csv(\"second_layer_results_10k.csv\", index=False)\n",
    "    second_layer_metrics = calculate_metrics(df_layer2)\n",
    "\n",
    "    # Third Layer\n",
    "    df_layer3 = df_layer2[df_layer2[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "    df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n",
    "    \n",
    "    # Save third layer result\n",
    "    df_layer3.to_csv(\"third_layer_results_10k.csv\", index=False)\n",
    "    third_layer_metrics = calculate_metrics(df_layer3)\n",
    "    \n",
    "    # Combine results\n",
    "    df_combined = pd.concat([\n",
    "        df_layer1[df_layer1[\"Prediction\"]],\n",
    "        df_layer2[df_layer2[\"Prediction\"]],\n",
    "        df_layer3[df_layer3[\"Prediction\"]]\n",
    "    ])\n",
    "\n",
    "    # Final output\n",
    "    df_layer1[\"Prediction\"] = False\n",
    "    df_layer1.loc[df_layer1[df_layer1[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df_layer1.loc[df_layer2[df_layer2[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df_layer1.loc[df_layer3[df_layer3[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "\n",
    "    return first_layer_metrics, second_layer_metrics, third_layer_metrics, df_combined\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    first_metrics, second_metrics, third_metrics, combined_data = process_name_matching(\"combined_data_10k.csv\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for Each Layer:\")\n",
    "\n",
    "    # First Layer Confusion Matrix\n",
    "    print(\"\\nFirst Layer Confusion Matrix:\")\n",
    "    first_cm = first_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(first_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {first_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {first_metrics['precision']}\")\n",
    "    print(f\"Recall: {first_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {first_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {first_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Second Layer Confusion Matrix\n",
    "    print(\"\\nSecond Layer Confusion Matrix:\")\n",
    "    second_cm = second_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(second_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {second_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {second_metrics['precision']}\")\n",
    "    print(f\"Recall: {second_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {second_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {second_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Third Layer Confusion Matrix\n",
    "    print(\"\\nThird Layer Confusion Matrix:\")\n",
    "    third_cm = third_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(third_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(f\"Accuracy: {third_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {third_metrics['precision']}\")\n",
    "    print(f\"Recall: {third_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {third_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {third_metrics['roc_auc_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ff9d11a-1fcf-47c9-b57d-d88c50bd68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10k = pd.read_csv(\"combined_data_10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6394415-758c-46b6-a628-3f02366acaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0    5179\n",
       "1    4821\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_10k['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de9942-6b54-4147-a3f6-892e472e4fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## without csv of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7d71ab8-443c-4b36-85a8-ae63ae08e978",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with new predictions and indicator column:\n",
      "         id merchant_type                 created_at     status docType  \\\n",
      "0  47566417   UNORGANIZED  2024-01-01 12:53:41+00:00  ACTIVATED  PAN_NO   \n",
      "1  48465215   UNORGANIZED  2024-02-14 14:22:48+00:00  ACTIVATED  PAN_NO   \n",
      "2  48036361   UNORGANIZED  2024-01-29 16:40:52+00:00  ACTIVATED  PAN_NO   \n",
      "3  46063565   UNORGANIZED  2023-10-06 19:39:01+00:00  ACTIVATED  PAN_NO   \n",
      "4  45539504           DIY  2023-09-12 19:00:36+00:00  ACTIVATED  PAN_NO   \n",
      "\n",
      "               name                     pan_createdAt pan_status  \\\n",
      "0             DAILI         2024-01-01 07:36:53+00:00   APPROVED   \n",
      "1  UMESH KUMAR NAIK  2024-02-14 09:00:02.613000+00:00   APPROVED   \n",
      "2      KSHAMA GUPTA  2024-01-29 11:18:32.509000+00:00   APPROVED   \n",
      "3      AFJAL ANSARI         2023-10-06 14:20:17+00:00   APPROVED   \n",
      "4         AMIN ALAM         2023-09-12 13:38:57+00:00   APPROVED   \n",
      "\n",
      "          beneficiary_name bank_status  ... dsFlag  szScore flScore   SCORE  \\\n",
      "0                    DAILI      ACTIVE  ...    NaN      NaN     1.0  100.00   \n",
      "1              SUJAL KUMAR      ACTIVE  ...  False      NaN     0.0    1.72   \n",
      "2  JATASHANKAR ENTERPRISES      ACTIVE  ...  False      NaN     0.0   40.00   \n",
      "3            Afjal  Ansari      ACTIVE  ...    NaN      NaN     1.0  100.00   \n",
      "4               AMIN  ALAM      ACTIVE  ...    NaN      NaN     1.0  100.00   \n",
      "\n",
      "  labels  First_Layer_Score  First_Layer_Pass Prediction  Second_Layer_Score  \\\n",
      "0      1             1.0000              True       True                 NaN   \n",
      "1      0             0.5750             False      False            0.476628   \n",
      "2      0             0.4100             False      False            0.133291   \n",
      "3      1             0.6225             False       True            1.000001   \n",
      "4      1             0.9600              True       True                 NaN   \n",
      "\n",
      "   Second_Layer_Pass  \n",
      "0                NaN  \n",
      "1              False  \n",
      "2              False  \n",
      "3               True  \n",
      "4                NaN  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        10146            1\n",
      "Actual 1          402         9953\n",
      "\n",
      "Correlation Matrix:\n",
      "                          id  szFlag   dsScore  szScore   flScore     SCORE  \\\n",
      "id                  1.000000     NaN -0.027826      NaN  0.019066  0.020089   \n",
      "szFlag                   NaN     NaN       NaN      NaN       NaN       NaN   \n",
      "dsScore            -0.027826     NaN  1.000000      NaN  0.100611  0.998056   \n",
      "szScore                  NaN     NaN       NaN      NaN       NaN       NaN   \n",
      "flScore             0.019066     NaN  0.100611      NaN  1.000000  0.893569   \n",
      "SCORE               0.020089     NaN  0.998056      NaN  0.893569  1.000000   \n",
      "labels              0.017274     NaN  0.480417      NaN  0.922784  0.924058   \n",
      "First_Layer_Score   0.006455     NaN  0.449603      NaN  0.862994  0.850939   \n",
      "Second_Layer_Score  0.019143     NaN  0.385134      NaN  0.836954  0.814584   \n",
      "\n",
      "                      labels  First_Layer_Score  Second_Layer_Score  \n",
      "id                  0.017274           0.006455            0.019143  \n",
      "szFlag                   NaN                NaN                 NaN  \n",
      "dsScore             0.480417           0.449603            0.385134  \n",
      "szScore                  NaN                NaN                 NaN  \n",
      "flScore             0.922784           0.862994            0.836954  \n",
      "SCORE               0.924058           0.850939            0.814584  \n",
      "labels              1.000000           0.858646            0.863434  \n",
      "First_Layer_Score   0.858646           1.000000            0.772596  \n",
      "Second_Layer_Score  0.863434           0.772596            1.000000  \n",
      "\n",
      "Metrics:\n",
      "Accuracy: 0.9803433811335479\n",
      "Precision: 0.9998995378742214\n",
      "Recall: 0.9611781747947852\n",
      "F1 Score: 0.9801565808262347\n",
      "AUC: 0.980539811749418\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "# def process_name_matching(file_path, num_rows=10):\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    # df=df.head(10)\n",
    "    # df = pd.read_csv(file_path, nrows=num_rows)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "\n",
    "    df[\"First_Layer_Score\"] = df.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df[\"First_Layer_Pass\"] = df[\"First_Layer_Score\"] >= threshold\n",
    "    df_layer2 = df[~df[\"First_Layer_Pass\"]].copy()\n",
    "\n",
    "    \n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "    df_layer3 = df_layer2[~df_layer2[\"Second_Layer_Pass\"]].copy()\n",
    "\n",
    "    \n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    df_combined = pd.concat([\n",
    "        df[df[\"First_Layer_Pass\"]],\n",
    "        df_layer2[df_layer2[\"Second_Layer_Pass\"]],\n",
    "        df_layer3[df_layer3[\"Third_Layer_Pass\"]],\n",
    "\n",
    "    ])\n",
    "    \n",
    "    df_combined.to_csv(\"layer_combine.csv\",index=False)\n",
    "    df[\"Prediction\"] = False\n",
    "    df.loc[df[\"First_Layer_Pass\"], \"Prediction\"] = True\n",
    "    df.loc[df_layer2.index[df_layer2[\"Second_Layer_Pass\"]], \"Prediction\"] = True\n",
    "    df.loc[df_layer3.index[df_layer3[\"Third_Layer_Pass\"]], \"Prediction\"] = True\n",
    "\n",
    "\n",
    "\n",
    "    df[\"First_Layer_Score\"] = df.get(\"First_Layer_Score\", None)\n",
    "    df[\"First_Layer_Pass\"] = df.get(\"First_Layer_Pass\", None)\n",
    "    df[\"Second_Layer_Score\"] = df_layer2.get(\"Second_Layer_Score\", None)\n",
    "    df[\"Second_Layer_Pass\"] = df_layer2.get(\"Second_Layer_Pass\", None)\n",
    "    # df[\"Third_Layer_Pass\"] = df_layer3.get(\"Third_Layer_Pass\", None)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    df.to_csv('last_result.csv')\n",
    "\n",
    "    \n",
    "    y_true = df['labels']\n",
    "    y_pred = df['Prediction']\n",
    "    y_scores = df['Prediction'].astype(float) \n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "        \"classification_report\": classification_report(y_true, y_pred, output_dict=True)\n",
    "    }\n",
    "\n",
    "    correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "    print(\"Updated DataFrame with new predictions and indicator column:\")\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "    return metrics, df, df_combined, correlation_matrix\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # nrows = 10 \n",
    "    # metrics, full_data, combined_data, correlation_matrix = process_name_matching(\"20k_balaned_data.csv\", num_rows=nrows)\n",
    "    metrics, full_data, combined_data, correlation_matrix = process_name_matching(\"combined_data_20k.csv\")\n",
    "    cm = metrics[\"confusion_matrix\"]\n",
    "    cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_df)\n",
    "\n",
    "    print(\"\\nCorrelation Matrix:\")\n",
    "    print(correlation_matrix)\n",
    "\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']}\")\n",
    "    print(f\"Precision: {metrics['precision']}\")\n",
    "    print(f\"Recall: {metrics['recall']}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']}\")\n",
    "    print(f\"AUC: {metrics['roc_auc_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdd97b-5db5-4a89-b667-89d4a5a3ff8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd9a69-d9ce-481c-9214-db4363707b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a02bbd-469a-4ee4-8cc8-bfa5a5378c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edbd4e-6d9c-4f35-a24c-02f19c4d5040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d5755-3ff2-4bee-9243-1fe5f6794d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae26e5-6b00-4018-b981-ce9dc4fdf37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d00db7-f1c4-48c2-aa12-384eb11cfe71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e1256-d19d-4652-9f89-52f8119e4608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4a34b42-7590-47fe-bb51-c8b3467489f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Layer Metrics:\n",
      "{'accuracy': 0.8001, 'precision': 0.9996458923512748, 'recall': 0.5855631611698817, 'f1_score': 0.7385219097449313, 'roc_auc_score': 0.7926850368506292, 'confusion_matrix': [[5178, 1], [1998, 2823]], 'classification_report': {'0': {'precision': 0.7215719063545151, 'recall': 0.9998069125313768, 'f1-score': 0.8382031566167544, 'support': 5179.0}, '1': {'precision': 0.9996458923512748, 'recall': 0.5855631611698817, 'f1-score': 0.7385219097449313, 'support': 4821.0}, 'accuracy': 0.8001, 'macro avg': {'precision': 0.8606088993528949, 'recall': 0.7926850368506293, 'f1-score': 0.7883625331808428, 'support': 10000.0}, 'weighted avg': {'precision': 0.855631375003553, 'recall': 0.8001, 'f1-score': 0.7901468274998484, 'support': 10000.0}}}\n",
      "\n",
      "First Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5178            1\n",
      "Actual 1         1998         2823\n",
      "\n",
      "First Layer Metrics Report:\n",
      "Accuracy: 0.8001\n",
      "Precision: 0.9996458923512748\n",
      "Recall: 0.5855631611698817\n",
      "F1 Score: 0.7385219097449313\n",
      "AUC: 0.7926850368506292\n",
      "\n",
      "Second Layer Metrics:\n",
      "{'accuracy': 0.9480211817168339, 'precision': 0.9993853718500307, 'recall': 0.8138138138138138, 'f1_score': 0.8971034482758621, 'roc_auc_score': 0.9068103445276099, 'confusion_matrix': [[5177, 1], [372, 1626]], 'classification_report': {'0': {'precision': 0.9329608938547486, 'recall': 0.999806875241406, 'f1-score': 0.965227929523632, 'support': 5178.0}, '1': {'precision': 0.9993853718500307, 'recall': 0.8138138138138138, 'f1-score': 0.8971034482758621, 'support': 1998.0}, 'accuracy': 0.9480211817168339, 'macro avg': {'precision': 0.9661731328523897, 'recall': 0.9068103445276099, 'f1-score': 0.931165688899747, 'support': 7176.0}, 'weighted avg': {'precision': 0.9514553346343714, 'recall': 0.9480211817168339, 'f1-score': 0.9462601600792279, 'support': 7176.0}}}\n",
      "\n",
      "Second Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5177            1\n",
      "Actual 1          372         1626\n",
      "\n",
      "Second Layer Metrics Report:\n",
      "Accuracy: 0.9480211817168339\n",
      "Precision: 0.9993853718500307\n",
      "Recall: 0.8138138138138138\n",
      "F1 Score: 0.8971034482758621\n",
      "AUC: 0.9068103445276099\n",
      "\n",
      "Third Layer Metrics:\n",
      "{'accuracy': 0.9598125788430348, 'precision': 0.9209039548022598, 'recall': 0.4381720430107527, 'f1_score': 0.5938069216757741, 'roc_auc_score': 0.7177338870645805, 'confusion_matrix': [[5163, 14], [209, 163]], 'classification_report': {'0': {'precision': 0.9610945644080418, 'recall': 0.9972957311184083, 'f1-score': 0.9788605555028913, 'support': 5177.0}, '1': {'precision': 0.9209039548022598, 'recall': 0.4381720430107527, 'f1-score': 0.5938069216757741, 'support': 372.0}, 'accuracy': 0.9598125788430348, 'macro avg': {'precision': 0.9409992596051509, 'recall': 0.7177338870645805, 'f1-score': 0.7863337385893328, 'support': 5549.0}, 'weighted avg': {'precision': 0.9584002218646375, 'recall': 0.9598125788430348, 'f1-score': 0.9530469040731405, 'support': 5549.0}}}\n",
      "\n",
      "Third Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5163           14\n",
      "Actual 1          209          163\n",
      "\n",
      "Third Layer Metrics Report:\n",
      "Accuracy: 0.9598125788430348\n",
      "Precision: 0.9209039548022598\n",
      "Recall: 0.4381720430107527\n",
      "F1 Score: 0.5938069216757741\n",
      "AUC: 0.7177338870645805\n",
      "\n",
      "Final Metrics:\n",
      "{'accuracy': 0.8528, 'precision': 0.973156258830178, 'recall': 0.7143746110765401, 'f1_score': 0.8239234449760766, 'roc_auc_score': 0.8480156507786639, 'confusion_matrix': [[5084, 95], [1377, 3444]], 'classification_report': {'0': {'precision': 0.7868750967342517, 'recall': 0.9816566904807877, 'f1-score': 0.8735395189003436, 'support': 5179.0}, '1': {'precision': 0.973156258830178, 'recall': 0.7143746110765401, 'f1-score': 0.8239234449760766, 'support': 4821.0}, 'accuracy': 0.8528, 'macro avg': {'precision': 0.8800156777822148, 'recall': 0.8480156507786639, 'f1-score': 0.8487314819382101, 'support': 10000.0}, 'weighted avg': {'precision': 0.8766812449806978, 'recall': 0.8528, 'f1-score': 0.8496196096614546, 'support': 10000.0}}}\n",
      "\n",
      "Final Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5084           95\n",
      "Actual 1         1377         3444\n",
      "\n",
      "Final Metrics Report:\n",
      "Accuracy: 0.8528\n",
      "Precision: 0.973156258830178\n",
      "Recall: 0.7143746110765401\n",
      "F1 Score: 0.8239234449760766\n",
      "AUC: 0.8480156507786639\n",
      "\n",
      "Correlation Matrix:\n",
      "               id  szFlag   dsScore  szScore   flScore     SCORE    labels\n",
      "id       1.000000     NaN -0.034166      NaN -0.011362 -0.011125 -0.008857\n",
      "szFlag        NaN     NaN       NaN      NaN       NaN       NaN       NaN\n",
      "dsScore -0.034166     NaN  1.000000      NaN  0.188749  0.998398  0.644348\n",
      "szScore       NaN     NaN       NaN      NaN       NaN       NaN       NaN\n",
      "flScore -0.011362     NaN  0.188749      NaN  1.000000  0.864661  0.885987\n",
      "SCORE   -0.011125     NaN  0.998398      NaN  0.864661  1.000000  0.912877\n",
      "labels  -0.008857     NaN  0.644348      NaN  0.885987  0.912877  1.000000\n",
      "\n",
      "Confusion Matrix for Each Layer:\n",
      "\n",
      "First Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5178            1\n",
      "Actual 1         1998         2823\n",
      "\n",
      "Second Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5177            1\n",
      "Actual 1          372         1626\n",
      "\n",
      "Third Layer Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5163           14\n",
      "Actual 1          209          163\n",
      "\n",
      "Final Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0         5084           95\n",
      "Actual 1         1377         3444\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "def calculate_metrics(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    y_true = df['labels']  # True labels\n",
    "    y_pred = df['Prediction']  # Predicted labels\n",
    "    y_scores = df['Prediction'].astype(float)  # Convert boolean to float for AUC calculation\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "        \"classification_report\": classification_report(y_true, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    # First Layer\n",
    "    df[\"First_Layer_Score\"] = df.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df[\"First_Layer_Pass\"] = df[\"First_Layer_Score\"] >= threshold\n",
    "    df[\"Prediction\"] = df[\"First_Layer_Pass\"]\n",
    "    df.to_csv(\"first_layer_results.csv\", index=False)\n",
    "    first_layer_metrics = calculate_metrics(\"first_layer_results.csv\")\n",
    "    \n",
    "    print(\"\\nFirst Layer Metrics:\")\n",
    "    print(first_layer_metrics)\n",
    "    first_cm = first_layer_metrics[\"confusion_matrix\"]\n",
    "    print(\"\\nFirst Layer Confusion Matrix:\")\n",
    "    print(pd.DataFrame(first_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(\"\\nFirst Layer Metrics Report:\")\n",
    "    print(f\"Accuracy: {first_layer_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {first_layer_metrics['precision']}\")\n",
    "    print(f\"Recall: {first_layer_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {first_layer_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {first_layer_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Second Layer\n",
    "    df_layer2 = pd.read_csv(\"first_layer_results.csv\")  # Read first layer output\n",
    "    df_layer2 = df_layer2[df_layer2[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "    df_layer2[\"Prediction\"] = df_layer2[\"Second_Layer_Pass\"]\n",
    "    df_layer2.to_csv(\"second_layer_results.csv\", index=False)\n",
    "    second_layer_metrics = calculate_metrics(\"second_layer_results.csv\")\n",
    "    \n",
    "    print(\"\\nSecond Layer Metrics:\")\n",
    "    print(second_layer_metrics)\n",
    "    second_cm = second_layer_metrics[\"confusion_matrix\"]\n",
    "    print(\"\\nSecond Layer Confusion Matrix:\")\n",
    "    print(pd.DataFrame(second_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(\"\\nSecond Layer Metrics Report:\")\n",
    "    print(f\"Accuracy: {second_layer_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {second_layer_metrics['precision']}\")\n",
    "    print(f\"Recall: {second_layer_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {second_layer_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {second_layer_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Third Layer\n",
    "    df_layer3 = pd.read_csv(\"second_layer_results.csv\")  # Read second layer output\n",
    "    df_layer3 = df_layer3[df_layer3[\"Prediction\"] == False]  # Filter rows where Prediction == False\n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "    df_layer3[\"Prediction\"] = df_layer3[\"Third_Layer_Pass\"]\n",
    "    df_layer3.to_csv(\"third_layer_results.csv\", index=False)\n",
    "    third_layer_metrics = calculate_metrics(\"third_layer_results.csv\")\n",
    "    \n",
    "    print(\"\\nThird Layer Metrics:\")\n",
    "    print(third_layer_metrics)\n",
    "    third_cm = third_layer_metrics[\"confusion_matrix\"]\n",
    "    print(\"\\nThird Layer Confusion Matrix:\")\n",
    "    print(pd.DataFrame(third_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(\"\\nThird Layer Metrics Report:\")\n",
    "    print(f\"Accuracy: {third_layer_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {third_layer_metrics['precision']}\")\n",
    "    print(f\"Recall: {third_layer_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {third_layer_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {third_layer_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Combine results\n",
    "    df_first_layer = pd.read_csv(\"first_layer_results.csv\")\n",
    "    df_second_layer = pd.read_csv(\"second_layer_results.csv\")\n",
    "    df_third_layer = pd.read_csv(\"third_layer_results.csv\")\n",
    "    df_combined = pd.concat([\n",
    "        df_first_layer[df_first_layer[\"Prediction\"]],\n",
    "        df_second_layer[df_second_layer[\"Prediction\"]],\n",
    "        df_third_layer[df_third_layer[\"Prediction\"]]\n",
    "    ])\n",
    "    df_combined.to_csv(\"combined_layer_results.csv\", index=False)\n",
    "\n",
    "    # Final output\n",
    "    df = pd.read_csv(file_path)  # Reload the original file\n",
    "    df[\"Prediction\"] = False\n",
    "    df.loc[df_first_layer[df_first_layer[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df.loc[df_second_layer[df_second_layer[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df.loc[df_third_layer[df_third_layer[\"Prediction\"]].index, \"Prediction\"] = True\n",
    "    df.to_csv(\"final_results.csv\", index=False)\n",
    "\n",
    "    # Final Metrics\n",
    "    final_metrics = calculate_metrics(\"final_results.csv\")\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    print(final_metrics)\n",
    "    final_cm = final_metrics[\"confusion_matrix\"]\n",
    "    print(\"\\nFinal Confusion Matrix:\")\n",
    "    print(pd.DataFrame(final_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "    print(\"\\nFinal Metrics Report:\")\n",
    "    print(f\"Accuracy: {final_metrics['accuracy']}\")\n",
    "    print(f\"Precision: {final_metrics['precision']}\")\n",
    "    print(f\"Recall: {final_metrics['recall']}\")\n",
    "    print(f\"F1 Score: {final_metrics['f1_score']}\")\n",
    "    print(f\"AUC: {final_metrics['roc_auc_score']}\")\n",
    "\n",
    "    # Correlation Matrix for final results\n",
    "    correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "    print(\"\\nCorrelation Matrix:\")\n",
    "    print(correlation_matrix)\n",
    "\n",
    "    return first_layer_metrics, second_layer_metrics, third_layer_metrics, final_metrics, df_combined\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    first_metrics, second_metrics, third_metrics, final_metrics, combined_data = process_name_matching(\"combined_data_10k.csv\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for Each Layer:\")\n",
    "    print(\"\\nFirst Layer Confusion Matrix:\")\n",
    "    first_cm = first_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(first_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "\n",
    "    print(\"\\nSecond Layer Confusion Matrix:\")\n",
    "    second_cm = second_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(second_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "\n",
    "    print(\"\\nThird Layer Confusion Matrix:\")\n",
    "    third_cm = third_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(third_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n",
    "\n",
    "    print(\"\\nFinal Confusion Matrix:\")\n",
    "    final_cm = final_metrics[\"confusion_matrix\"]\n",
    "    print(pd.DataFrame(final_cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6567c-c85f-467a-8725-4e5ba3d9aa8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac2b09-5d39-4672-a11b-9722456d06ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa60bc5-38b1-4cfc-b863-3403bdc4eb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62b91e-033f-4be6-86d4-1eaa7edf0368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b88be9-4417-4f6c-9ff5-350dac909bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be555d3-0236-4339-a041-f03a40b48c87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## on 60k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8ccc6b8-7921-4445-9b19-c552eeb8d5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## on unseen 60k data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ff197269-c0b2-4fb3-a761-39d63131b822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_192092/3062873575.py:17: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with new predictions and indicator column:\n",
      "         id merchant_type                 created_at     status docType  \\\n",
      "0  50255782   UNORGANIZED  2024-06-15 18:49:10+00:00  ACTIVATED  PAN_NO   \n",
      "1  47708404           DIY  2024-01-08 21:42:28+00:00  ACTIVATED  PAN_NO   \n",
      "2  48873693   UNORGANIZED  2024-03-06 18:16:04+00:00  ACTIVATED  PAN_NO   \n",
      "3  48347356   UNORGANIZED  2024-02-10 14:59:20+00:00  ACTIVATED  PAN_NO   \n",
      "4  49433305   UNORGANIZED  2024-04-11 14:17:55+00:00  ACTIVATED  PAN_NO   \n",
      "\n",
      "                       name                     pan_createdAt pan_status  \\\n",
      "0  DHAARANI VELU RENUGADEVI  2024-06-15 13:44:18.424000+00:00   APPROVED   \n",
      "1                 KAMRUDDIN         2024-01-09 07:55:06+00:00   APPROVED   \n",
      "2                 MOHD AZAM  2024-03-06 12:53:41.360000+00:00   APPROVED   \n",
      "3                 SANA BANO  2024-02-10 09:34:53.174000+00:00   APPROVED   \n",
      "4          DHIRAJ KUMAR RAY  2024-04-11 08:54:19.945000+00:00   APPROVED   \n",
      "\n",
      "       beneficiary_name bank_status  ... szScore  flScore  SCORE labels  \\\n",
      "0           DHAARANI  V      ACTIVE  ...     NaN      0.5  100.0      1   \n",
      "1             KAMRUDDIN      ACTIVE  ...     NaN      1.0  100.0      1   \n",
      "2  MOHD AZAM S/O SHAMSH      ACTIVE  ...     NaN      1.0  100.0      1   \n",
      "3             Sana Bano      ACTIVE  ...     NaN      1.0  100.0      1   \n",
      "4      DHIRAJ KUMAR ROY      ACTIVE  ...     NaN      0.9   89.0      1   \n",
      "\n",
      "  First_Layer_Score  First_Layer_Pass  Prediction Second_Layer_Score  \\\n",
      "0             0.755             False        True           0.513418   \n",
      "1             1.000              True        True                NaN   \n",
      "2             0.810              True        True                NaN   \n",
      "3             0.665             False        True           1.000001   \n",
      "4             0.940              True        True                NaN   \n",
      "\n",
      "   Second_Layer_Pass  Third_Layer_Pass  \n",
      "0              False              True  \n",
      "1                NaN               NaN  \n",
      "2                NaN               NaN  \n",
      "3               True               NaN  \n",
      "4                NaN               NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0        30309         1292\n",
      "Actual 1         1281        29000\n",
      "\n",
      "Correlation Matrix:\n",
      "                          id  szFlag   dsScore  szScore   flScore     SCORE  \\\n",
      "id                  1.000000     NaN -0.022728      NaN  0.005622  0.008561   \n",
      "szFlag                   NaN     NaN       NaN      NaN       NaN       NaN   \n",
      "dsScore            -0.022728     NaN  1.000000      NaN  0.191965  0.997499   \n",
      "szScore                  NaN     NaN       NaN      NaN       NaN       NaN   \n",
      "flScore             0.005622     NaN  0.191965      NaN  1.000000  0.862594   \n",
      "SCORE               0.008561     NaN  0.997499      NaN  0.862594  1.000000   \n",
      "labels              0.012017     NaN  0.729772      NaN  0.892649  0.935509   \n",
      "First_Layer_Score   0.001388     NaN  0.592213      NaN  0.830018  0.837055   \n",
      "Second_Layer_Score  0.011947     NaN  0.524321      NaN  0.830690  0.837211   \n",
      "\n",
      "                      labels  First_Layer_Score  Second_Layer_Score  \n",
      "id                  0.012017           0.001388            0.011947  \n",
      "szFlag                   NaN                NaN                 NaN  \n",
      "dsScore             0.729772           0.592213            0.524321  \n",
      "szScore                  NaN                NaN                 NaN  \n",
      "flScore             0.892649           0.830018            0.830690  \n",
      "SCORE               0.935509           0.837055            0.837211  \n",
      "labels              1.000000           0.829559            0.843377  \n",
      "First_Layer_Score   0.829559           1.000000            0.781850  \n",
      "Second_Layer_Score  0.843377           0.781850            1.000000  \n",
      "\n",
      "Metrics:\n",
      "Accuracy: 0.9584208655182443\n",
      "Precision: 0.9573484748448435\n",
      "Recall: 0.9576962451702388\n",
      "F1 Score: 0.9575223284301587\n",
      "AUC: 0.9584057315215454\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "def process_name_matching(file_path):\n",
    "# def process_name_matching(file_path, num_rows=10):\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    # df=df.head(10)\n",
    "    # df = pd.read_csv(file_path, nrows=num_rows)\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "\n",
    "    df[\"First_Layer_Score\"] = df.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df[\"First_Layer_Pass\"] = df[\"First_Layer_Score\"] >= threshold\n",
    "    df_layer2 = df[~df[\"First_Layer_Pass\"]].copy()\n",
    "\n",
    "    \n",
    "#     df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "#     df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "#     df_layer3 = df_layer2[~df_layer2[\"Second_Layer_Pass\"]].copy()\n",
    "\n",
    "    \n",
    "#     df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "\n",
    "    df_layer2[\"Second_Layer_Score\"] = df_layer2.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "\n",
    "    # Determine if Second Layer passes\n",
    "    df_layer2[\"Second_Layer_Pass\"] = df_layer2[\"Second_Layer_Score\"] >= threshold1\n",
    "\n",
    "    # Filter out fail cases for the third layer\n",
    "    df_layer3 = df_layer2[~df_layer2[\"Second_Layer_Pass\"]].copy()\n",
    "\n",
    "    # Validation: Ensure all fail cases are passed to the third layer\n",
    "    fail_cases_in_layer2 = df_layer2[~df_layer2[\"Second_Layer_Pass\"]]\n",
    "    assert fail_cases_in_layer2.shape[0] == df_layer3.shape[0], \"Not all fail cases are passed to the third layer!\"\n",
    "    assert set(fail_cases_in_layer2.index) == set(df_layer3.index), \"Mismatch in row indices between failing cases and third layer!\"\n",
    "\n",
    "    # Process Third Layer on fail cases\n",
    "    df_layer3[\"Third_Layer_Pass\"] = df_layer3.apply(process_false_cases, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    df_combined = pd.concat([\n",
    "        df[df[\"First_Layer_Pass\"]],\n",
    "        df_layer2[df_layer2[\"Second_Layer_Pass\"]],\n",
    "        df_layer3[df_layer3[\"Third_Layer_Pass\"]],\n",
    "\n",
    "    ])\n",
    "    \n",
    "    df_combined.to_csv(\"weight_combine.csv\",index=False)\n",
    "    df[\"Prediction\"] = False\n",
    "    df.loc[df[\"First_Layer_Pass\"], \"Prediction\"] = True\n",
    "    df.loc[df_layer2.index[df_layer2[\"Second_Layer_Pass\"]], \"Prediction\"] = True\n",
    "    df.loc[df_layer3.index[df_layer3[\"Third_Layer_Pass\"]], \"Prediction\"] = True\n",
    "\n",
    "\n",
    "\n",
    "    df[\"First_Layer_Score\"] = df.get(\"First_Layer_Score\", None)\n",
    "    df[\"First_Layer_Pass\"] = df.get(\"First_Layer_Pass\", None)\n",
    "    df[\"Second_Layer_Score\"] = df_layer2.get(\"Second_Layer_Score\", None)\n",
    "    df[\"Second_Layer_Pass\"] = df_layer2.get(\"Second_Layer_Pass\", None)\n",
    "    df[\"Third_Layer_Pass\"] = df_layer3.get(\"Third_Layer_Pass\", None)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    df.to_csv('layer_weight.csv')\n",
    "\n",
    "    \n",
    "    y_true = df['labels']\n",
    "    y_pred = df['Prediction']\n",
    "    y_scores = df['Prediction'].astype(float) \n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_scores),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "        \"classification_report\": classification_report(y_true, y_pred, output_dict=True)\n",
    "    }\n",
    "\n",
    "    correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "    print(\"Updated DataFrame with new predictions and indicator column:\")\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "    return metrics, df, df_combined, correlation_matrix\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # nrows = 10 \n",
    "    # metrics, full_data, combined_data, correlation_matrix = process_name_matching(\"20k_balaned_data.csv\", num_rows=nrows)\n",
    "    metrics, full_data, combined_data, correlation_matrix = process_name_matching(\"combined_data_60k.csv\")\n",
    "    cm = metrics[\"confusion_matrix\"]\n",
    "    cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_df)\n",
    "\n",
    "    print(\"\\nCorrelation Matrix:\")\n",
    "    print(correlation_matrix)\n",
    "\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']}\")\n",
    "    print(f\"Precision: {metrics['precision']}\")\n",
    "    print(f\"Recall: {metrics['recall']}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']}\")\n",
    "    print(f\"AUC: {metrics['roc_auc_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca01864-90fd-4c1e-852c-5064a468faaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae153b-7bd4-4464-b836-fafe85ea9e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b6da5db-f946-4ec2-ac05-0a0e3ff7fc07",
   "metadata": {
    "tags": []
   },
   "source": [
    "## name match function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35fedf03-b1a0-49a0-a0c4-04316e219eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result DataFrame with layer results and prediction:\n",
      "         name1           name2  First_Layer_Score  First_Layer_Pass  \\\n",
      "0  Sara Trader  Sara Tendulkar               0.64             False   \n",
      "\n",
      "   Second_Layer_Score  Second_Layer_Pass  Third_Layer_Pass  Prediction  \n",
      "0            0.571201              False             False       False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_name_matching_from_names(name1, name2):\n",
    "    \"\"\"\n",
    "    Process name matching through multiple layers with given name1 and name2.\n",
    "    Returns a DataFrame with layer scores and final prediction.\n",
    "    \"\"\"\n",
    "    # Define thresholds\n",
    "    threshold = 0.80\n",
    "    threshold1 = 0.65\n",
    "\n",
    "    # Create a DataFrame with the given names\n",
    "    data = {\n",
    "        'name1': [name1],\n",
    "        'name2': [name2],\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # First Layer: Fuzzy Similarity\n",
    "    df[\"First_Layer_Score\"] = df.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df[\"First_Layer_Pass\"] = df[\"First_Layer_Score\"] >= threshold\n",
    "\n",
    "    # If the first layer passes, set prediction and return the DataFrame\n",
    "    if df[\"First_Layer_Pass\"].iloc[0]:\n",
    "        df[\"Prediction\"] = True\n",
    "        return df\n",
    "\n",
    "    # Second Layer: Name Match\n",
    "    df[\"Second_Layer_Score\"] = df.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    df[\"Second_Layer_Pass\"] = df[\"Second_Layer_Score\"] >= threshold1\n",
    "\n",
    "    # If the second layer passes, set prediction and return the DataFrame\n",
    "    if df[\"Second_Layer_Pass\"].iloc[0]:\n",
    "        df[\"Prediction\"] = True\n",
    "        return df\n",
    "\n",
    "    # Third Layer: Handle False Cases\n",
    "    df[\"Third_Layer_Pass\"] = df.apply(process_false_cases, axis=1)\n",
    "\n",
    "    # Final Prediction\n",
    "    df[\"Prediction\"] = df[\"Third_Layer_Pass\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    name1 = \"Sara Trader\"\n",
    "    name2 = \"Sara Tendulkar\"\n",
    "    df_result = process_name_matching_from_names(name1, name2)\n",
    "    print(\"Result DataFrame with layer results and prediction:\")\n",
    "    print(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05171a-dc3e-43b0-87fa-0bf122b62979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "066a6963-450e-4757-a8ad-8bfb36f7ae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame with layer results, fuzzy_SS, and updated predictions:\n",
      "          name1 name2  label  First_Layer_Score  First_Layer_Pass  \\\n",
      "0  Sara Trader  Sara      0              0.765             False   \n",
      "\n",
      "   Second_Layer_Score  Second_Layer_Pass  Third_Layer_Score  Prediction  \\\n",
      "0            0.812393               True                1.0       False   \n",
      "\n",
      "  fuzzy_SS  \n",
      "0    0.765  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_name_matching_from_names(name1, name2):\n",
    "    threshold = 0.80\n",
    "\n",
    "    # Initial data setup\n",
    "    data = {\n",
    "        'name1': [name1],\n",
    "        'name2': [name2],\n",
    "        # 'label': [0],  # Assuming 'label' is set to 1 as specified\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # First Layer - Fuzzy Similarity\n",
    "    df[\"First_Layer_Score\"] = df.apply(lambda x: calculate_fuzzy_similarity(x['name1'], x['name2']), axis=1)\n",
    "    df[\"First_Layer_Pass\"] = df[\"First_Layer_Score\"] >= threshold\n",
    "\n",
    "    # Second Layer - Name Match\n",
    "    name_match_result = df.apply(lambda x: name_match(x['name1'], x['name2']), axis=1)\n",
    "    \n",
    "    # Check if name_match_result has multiple columns\n",
    "    if not name_match_result.empty and isinstance(name_match_result.iloc[0], pd.Series):\n",
    "        for i, col_name in enumerate(name_match_result.iloc[0].index):\n",
    "            df[f\"Second_Layer_Score_{col_name}\"] = name_match_result.apply(lambda x: x[i])\n",
    "        df[\"Second_Layer_Score\"] = name_match_result.apply(lambda x: x[0])\n",
    "    else:\n",
    "        df[\"Second_Layer_Score\"] = name_match_result\n",
    "\n",
    "    df[\"Second_Layer_Pass\"] = df[\"Second_Layer_Score\"] >= threshold\n",
    "\n",
    "    # Third Layer - Initial Matching\n",
    "    df[\"Third_Layer_Score\"] = df.apply(lambda x: check_initial_similarity(x['name1'], x['name2']), axis=1)\n",
    "\n",
    "    # Initialize Prediction column with False\n",
    "    df[\"Prediction\"] = False\n",
    "\n",
    "    # Initialize fuzzy_SS column with None for all rows\n",
    "    df[\"fuzzy_SS\"] = None\n",
    "\n",
    "    # Update Prediction for rows that pass each layer\n",
    "    df.loc[df[\"First_Layer_Pass\"], \"Prediction\"] = True\n",
    "    df.loc[df.get(\"Second_Layer_Pass\", pd.Series([False] * len(df))), \"Prediction\"] = True\n",
    "    df.loc[df.get(\"Third_Layer_Pass\", pd.Series([False] * len(df))), \"Prediction\"] = True\n",
    "\n",
    "    # Additional processing for rows failing all layers\n",
    "    failed_cases = df[(df['label'] == 0) & (df['Prediction'] == True)]\n",
    "\n",
    "    # Run additional fuzzy similarity checks on failed cases\n",
    "    for index, row in failed_cases.iterrows():\n",
    "        name1 = row['name1']\n",
    "        name2 = row['name2']\n",
    "        fuzzy_SS = calculate_fuzzy_similarity_processed(name1, name2)\n",
    "\n",
    "        # Set fuzzy_SS for failed cases\n",
    "        df.at[index, 'fuzzy_SS'] = fuzzy_SS\n",
    "\n",
    "        # Determine fuzzy_flag based on threshold and keywords\n",
    "        fuzzy_flag = 1 if fuzzy_SS >= threshold else 0\n",
    "        if fuzzy_flag == 1:\n",
    "            fuzzy_flag = check_keywords_in_names(name1, name2)\n",
    "\n",
    "        prediction_value = True if fuzzy_flag == 1 else False\n",
    "\n",
    "        # Update failed cases directly in Prediction\n",
    "        df.at[index, 'Prediction'] = prediction_value\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # name1 = \"DHANUSH MUKESH PATEL\"\n",
    "    # name2 = \"C M INFOSYS\"\n",
    "    name1 = \"Sara Trader\"\n",
    "    name2 = \"Sara\"\n",
    "    df_combined = process_name_matching_from_names(name1, name2)\n",
    "    print(\"Final DataFrame with layer results, fuzzy_SS, and updated predictions:\\n\", df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a28829-351c-42d5-957f-7f36e6feaf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name1</th>\n",
       "      <th>name2</th>\n",
       "      <th>label</th>\n",
       "      <th>First_Layer_Score</th>\n",
       "      <th>First_Layer_Pass</th>\n",
       "      <th>Second_Layer_Score</th>\n",
       "      <th>Second_Layer_Pass</th>\n",
       "      <th>Third_Layer_Score</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>fuzzy_SS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sara Trader</td>\n",
       "      <td>Sara</td>\n",
       "      <td>0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>False</td>\n",
       "      <td>0.812393</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name1 name2  label  First_Layer_Score  First_Layer_Pass  \\\n",
       "0  Sara Trader  Sara      0              0.765             False   \n",
       "\n",
       "   Second_Layer_Score  Second_Layer_Pass  Third_Layer_Score  Prediction  \\\n",
       "0            0.812393               True                1.0       False   \n",
       "\n",
       "  fuzzy_SS  \n",
       "0    0.765  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47b9a8-587a-4c5c-b575-619d9e30eda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a170dab-f73d-40ae-b6b1-27a6c8c7ef69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ec5db-3351-4a53-8c3c-1e0775f5de4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "namematchenv",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Name_Match",
   "language": "python",
   "name": "namematchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
